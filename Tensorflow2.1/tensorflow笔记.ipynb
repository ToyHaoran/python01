{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ],
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 张量 精度\n",
    "维数\t阶\t   名字    \t例子\n",
    "0-D\t    0\t标量 scalar\ts=1 2 3\n",
    "1-D\t    1\t向量 vector\tv=[1, 2, 3]\n",
    "2-D\t    2\t矩阵 matrix\tm=[[1,2,3],[4,5,6],[7,8,9]]\n",
    "N-D\t    N\t张量 tensor\tt=[[[ 有几个中括号就是几阶张量\n",
    "\n",
    "张量类型|精度：\n",
    "- tf.int32(默认), tf.float32, 深度学习足够了\n",
    "- tf.float64,  强化学习可以用\n",
    "- tf.bool, tf.string\n",
    "\n",
    "## 创建张量 (传入list)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "a = tf.constant([1, 5], dtype=tf.int64)  # 指定保存精度\n",
    "b = tf.cast(a, dtype=tf.int32)  # 转换精度和类型\n",
    "print(b)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "a = tf.constant([True, True, False])\n",
    "b = tf.cast(a, dtype=tf.int32)\n",
    "print(b)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 将Numpy数组转为Tensor\n",
    "a = np.arange(0, 5)\n",
    "b = tf.convert_to_tensor(a, dtype=tf.int32)\n",
    "print(\"a:\", a)\n",
    "print(\"转为张量:\", b)\n",
    "print(\"转为Numpy：\", b.numpy())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 填充张量(维度,指定值)\n",
    "一维直接写个数  二维用[行，列]  多维用[n,m,.K....]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "a = tf.zeros([2, 3])\n",
    "b = tf.ones(4)\n",
    "c = tf.fill([2, 2], 9)\n",
    "d = tf.zeros_like(c)  # tf.xx_like()与tf.zeros(c.shape)等价\n",
    "print(\"a:\", a)\n",
    "print(\"b:\", b)\n",
    "print(\"c:\", c)\n",
    "print(d)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "a = tf.range(10, delta=2)\n",
    "print(a)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 数学运算\n",
    "对应元素的四则运算(必须维度相同): TensorFlow已经重载了+ − ∗ / // % **运算符\n",
    "平方与开方: tf.square  tf.sqrt\n",
    "矩阵乘: tf.matmul(矩阵1, 矩阵2)\n",
    "其他：exp(x) tf.math.log(x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 理解axis\n",
    "在一个二维张量或数组中，可以通过调整axis等于0或1控制执行维度。\n",
    "axis=0代表跨行(经度，down),而axis=1代表跨列(纬度，across)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tf.Tensor(\n",
      "[[1 2 3]\n",
      " [2 2 3]], shape=(2, 3), dtype=int32)\n",
      "所有数的均值: tf.Tensor(2, shape=(), dtype=int32)\n",
      "每一行的和: tf.Tensor([6 7], shape=(2,), dtype=int32)\n",
      "每行最小值： tf.Tensor([1 2], shape=(2,), dtype=int32)\n",
      "每列最大值: tf.Tensor([2 2 3], shape=(3,), dtype=int32)\n",
      "每列最大值的索引： tf.Tensor([1 0 0], shape=(3,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant([[1, 2, 3], [2, 2, 3]])\n",
    "print(\"x:\", x)\n",
    "print(\"所有数的均值:\", tf.reduce_mean(x))  # 不指定axis,则所有元素参与计算\n",
    "print(\"每一行的和:\", tf.reduce_sum(x, axis=1))\n",
    "print(\"每行最小值：\", tf.reduce_min(x, axis=1))\n",
    "print(\"每列最大值:\", tf.reduce_max(x, axis=0))\n",
    "print(\"每列最大值的索引：\", tf.argmax(x, axis=0))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 索引和切片\n",
    "切片[start:end:step] 可根据需要省略；和列表切片一样，左闭右开"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([-0.34981516  0.65928495  0.6753863 ], shape=(3,), dtype=float32)\n",
      "(4, 32, 32) 3\n"
     ]
    }
   ],
   "source": [
    "x = tf.random.normal([4,32,32,3])  # 4张32x32大小的彩色图片(RGB)\n",
    "print(x[0,1,2])  # 取第1张图片，第2行，第3列的像素  x[0][1][2]\n",
    "x2 = x[1:3, ::2, ::-2]  # 表示取第2 3张图片，隔行采样，逆序隔列采样，所有通道信息\n",
    "x3 = x[..., 1]  # 读取所有图片的G通道\n",
    "print(x3.shape, x3.ndim)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 维度变换 | 广播机制"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x = tf.range(96)\n",
    "x = tf.reshape(x, [2, 4, 4, 3])  # 改变张量的视图(理解方式)，并不会改变张量的存储顺序\n",
    "x = tf.transpose(x, perm=[1,2,0,3])  # 交换维度(改变存储顺序) perm表示新维度的顺序\n",
    "x = tf.reshape(x, [16, 6])\n",
    "x = tf.expand_dims(x, axis=0) # 指定的axis轴前插入新的维度 (为负则是在其后插入)\n",
    "x = tf.squeeze(x, axis=1)  # axis参数为待删除的维度的索引号\n",
    "print(x.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 3)\n"
     ]
    }
   ],
   "source": [
    "a = tf.random.normal([3, 1])\n",
    "b = tf.random.normal([1, 3])\n",
    "c = a + b  # 广播机制\n",
    "print(c.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## where(条件语句，真返回A，假返回B)\n",
    "类似三元运算符"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c： tf.Tensor([1 2 3 4 5], shape=(5,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "a = tf.constant([1, 2, 3, 1, 1])\n",
    "b = tf.constant([0, 1, 3, 4, 5])\n",
    "c = tf.where(tf.greater(a, b), a, b)  # 若a>b，返回a对应位置的元素，否则返回b对应位置的元素\n",
    "print(\"c：\", c)  # [1 2 3 4 5]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 常用函数"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"传入特征与标签：切分传入张量的第一维度，生成输入特征标签对，构建数据集 (适用Numpy和Tensor)\"\"\"\n",
    "features = tf.constant([12, 23, 10, 17])\n",
    "labels = tf.constant([0, 1, 1, 0])\n",
    "dataset = tf.data.Dataset.from_tensor_slices((features, labels))  # (输入特征，标签)\n",
    "for element in dataset:\n",
    "    print(element)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "函数对指定参数求导，如y=x^2求导\n",
    "tf.Variable(初始值) 将变量标记为“可训练”，被标记的变量会在反向传播中记录梯度信息。\n",
    "w = tf.Variable(tf.random.normal([2, 2], mean=0, stddev=1))\n",
    "\"\"\"\n",
    "with tf.GradientTape() as tape:  # with结构记录计算过程\n",
    "    w = tf.Variable(tf.constant(tf.cast(range(4), tf.float32)))\n",
    "    loss = tf.pow(w, 2)\n",
    "grad = tape.gradient(loss, w)  # 求出张量的梯度=2w (函数，对谁求导)\n",
    "print(grad)  # tf.Tensor([0. 2. 4. 6.], shape=(4,), dtype=float32)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 独热编码\n",
    "独热编码(one-hot encoding) ：在分类问题中，常用独热码做标签，标记类别: 1表示是，0表示非。\n",
    "0狗尾草鸢尾 1杂色鸢尾 2弗吉尼亚鸢尾；\n",
    "标签1表示为[0. 1. 0.] 0%可能是0狗尾草鸢尾；100%可能是1杂色鸢尾；0%可能是2弗吉尼亚鸢尾"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "labels = tf.constant([1, 0, 2, 1])  # 输入的元素值最小为0，最大为2，共3分类\n",
    "output = tf.one_hot(labels, depth=3)  # (待转换数据，depth=几分类)\n",
    "print(\"result of labels1:\", output)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"softmax() 使n分类的n个输出(y0,y1,...yn-1)通过softmax()函数符合概率分布；\"\"\"\n",
    "y = tf.constant([1.01, 2.01, -0.66])\n",
    "y_pro = tf.nn.softmax(y)\n",
    "print(\"After softmax, y_pro is:\", y_pro)  # y_pro 符合概率分布\n",
    "print(\"The sum of y_pro:\", tf.reduce_sum(y_pro))  # 通过softmax后，所有概率加起来和为1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 待优化张量Variable\n",
    "由于梯度运算会消耗大量的计算资源，而且会自动更新相关参数；\n",
    "对于不需要的优化的张量，如神经网络的输入X，不需要通过tf.Variable封装；\n",
    "对于需要计算梯度并优化的张量，如神经网络层的W和𝒃，需要通过tf.Variable包裹以便TensorFlow跟踪相关梯度信息；"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "w = tf.Variable(4)\n",
    "w.assign_sub(1)  # (w要自减的内容) 即w=w-1\n",
    "print(\"x:\", w)  # 4-1=3"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 随机数生成\n",
    "np.random\n",
    "binomial() 二项分布  beta() Beta分布的样本值"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1  -4   7  -8  -7]\n",
      " [ -7   2   6   7  -5]\n",
      " [  3  -8   1   0 -10]]\n"
     ]
    }
   ],
   "source": [
    "rdm = np.random.RandomState(seed=12)  # 每次生成的随机数都是一样的\n",
    "print(rdm.randint(-10, 10, 15, np.int32).reshape(3, 5))  # 随机整数"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 正态分布"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.50059013  0.47841395]\n",
      " [-1.11256027 -1.31292783]]\n",
      "d: tf.Tensor(\n",
      "[[0.88427985 1.0029912 ]\n",
      " [1.6233026  0.55114424]], shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 生成正态分布的随机数，默认均值为0，标准差为1 (维度，mean=均值，stddev=标准差)\n",
    "d = np.random.randn(2, 2)  # 返回2*2的标准正态随机数\n",
    "d = np.random.normal(0.5, 1, [2, 2])\n",
    "d = tf.random.normal([2, 2], mean=0.5, stddev=1)\n",
    "print(\"d:\", d)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 生成截断式正态分布的随机数，取值在(μ-2σ， μ+2σ)之内 (维度，mean=均值，stddev=标准差)\n",
    "e = tf.random.truncated_normal([2, 2], mean=0.5, stddev=1)\n",
    "print(\"e:\", e)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 均匀分布"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.85719534 0.46885937]\n",
      " [0.2352003  0.05984159]]\n",
      "f: tf.Tensor(\n",
      "[[0.17577958 0.62613165]\n",
      " [0.6155919  0.31002963]], shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 生成均匀分布随机数 (维度，minval=最小值，maxval=最大值)\n",
    "f = np.random.rand(2,2)  # [0,1)之间的随机数\n",
    "f = np.random.uniform(0, 1, [2, 2])\n",
    "f = tf.random.uniform([2, 2], minval=0, maxval=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}