{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "import torchtext  # 内置的文本处理 pip install torchtext==0.6.0\n",
    "from torchtext.vocab import GloVe  # 词嵌入表示的库\n",
    "from torchtext.datasets import IMDB\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 文本预处理\n",
    "使用 时间机器.txt 数据集"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. 将文本作为字符串加载到内存中"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [
    "def read_txt(path):\n",
    "    \"\"\"读取txt文件并处理，返回文本列表\"\"\"\n",
    "    # 将时间机器数据集加载到文本行的列表中\n",
    "    with open(path, 'r', encoding='UTF-8') as f:\n",
    "        lines = f.readlines()\n",
    "    # 将非字母替换为空格，并全部小写\n",
    "    return [re.sub('[^A-Za-z]+', ' ', line).strip().lower() for line in lines]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "(3557,\n 'the project gutenberg ebook of the time machine by h g wells',\n 'length breadth thickness and duration but through a natural')"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = read_txt(\"../data/时间机器.txt\")\n",
    "len(lines),lines[0],lines[104]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. 将字符串拆分为词元token(如单词和字符)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [],
   "source": [
    "def tokenize(lines, token='word'):\n",
    "    if token == 'word':\n",
    "        return [line.split() for line in lines]\n",
    "    elif token == 'char':\n",
    "        return [list(line) for line in lines]\n",
    "    else:\n",
    "        print('错误：未知词元类型：' + token)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'project', 'gutenberg', 'ebook', 'of', 'the', 'time', 'machine', 'by', 'h', 'g', 'wells']\n",
      "[]\n",
      "['this', 'ebook', 'is', 'for', 'the', 'use', 'of', 'anyone', 'anywhere', 'in', 'the', 'united', 'states', 'and']\n",
      "['most', 'other', 'parts', 'of', 'the', 'world', 'at', 'no', 'cost', 'and', 'with', 'almost', 'no', 'restrictions']\n",
      "['whatsoever', 'you', 'may', 'copy', 'it', 'give', 'it', 'away', 'or', 're', 'use', 'it', 'under', 'the', 'terms']\n",
      "['of', 'the', 'project', 'gutenberg', 'license', 'included', 'with', 'this', 'ebook', 'or', 'online', 'at']\n",
      "['www', 'gutenberg', 'org', 'if', 'you', 'are', 'not', 'located', 'in', 'the', 'united', 'states', 'you']\n",
      "['will', 'have', 'to', 'check', 'the', 'laws', 'of', 'the', 'country', 'where', 'you', 'are', 'located', 'before']\n",
      "['using', 'this', 'ebook']\n",
      "[]\n",
      "['title', 'the', 'time', 'machine']\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenize(lines)\n",
    "for i in range(11):\n",
    "    print(tokens[i])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. 建立词表，将拆分的词元映射到数字索引"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [],
   "source": [
    "import collections\n",
    "def count_corpus(tokens):\n",
    "    \"\"\"统计词元的频率，tokens是1D或2D列表\"\"\"\n",
    "    # 这里使用了短路逻辑，避免tokens[0]越界\n",
    "    if len(tokens) == 0 or isinstance(tokens[0], list):\n",
    "        # 如果是2D列表，将词元列表展平成1D列表，嵌套循环\n",
    "        tokens = [token for line in tokens for token in line]\n",
    "    return collections.Counter(tokens)  # 单词:词频\n",
    "\n",
    "class Vocab:\n",
    "    \"\"\"文本词表\"\"\"\n",
    "    def __init__(self, tokens=None, min_freq=0, reserved_tokens=None):\n",
    "        \"\"\"\n",
    "        :param tokens: 传入的词元列表\n",
    "        :param min_freq: 少于该次数的词元丢掉\n",
    "        :param reserved_tokens: 已知的token\n",
    "        \"\"\"\n",
    "        if tokens is None:\n",
    "            tokens = []\n",
    "        if reserved_tokens is None:\n",
    "            reserved_tokens = []\n",
    "        counter = count_corpus(tokens)  # 统计词频\n",
    "        # 按出现频率排序，计算性能较好\n",
    "        self._token_freqs = sorted(counter.items(), key=lambda x: x[1], reverse=True)\n",
    "        # 列表，根据下标idx找到对应的token\n",
    "        self.idx_to_token = ['<unk>'] + reserved_tokens  # 未知词元<unk>的索引为0\n",
    "        # 字典，根据token找到对应的idx\n",
    "        self.token_to_idx = {token: idx for idx, token in enumerate(self.idx_to_token)}\n",
    "        for token, freq in self._token_freqs:\n",
    "            if freq < min_freq:\n",
    "                break\n",
    "            if token not in self.token_to_idx:  # 不在词表中\n",
    "                self.idx_to_token.append(token)\n",
    "                self.token_to_idx[token] = len(self.idx_to_token) - 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx_to_token)\n",
    "\n",
    "    def __getitem__(self, tokens):\n",
    "        \"\"\"字典，根据token找到对应的idx\"\"\"\n",
    "        if not isinstance(tokens, (list, tuple)):\n",
    "            return self.token_to_idx.get(tokens, self.unk)\n",
    "        return [self.__getitem__(token) for token in tokens]\n",
    "\n",
    "    def to_tokens(self, indices):\n",
    "        \"\"\"列表，根据下标idx找到对应的token\"\"\"\n",
    "        if not isinstance(indices, (list, tuple)):\n",
    "            return self.idx_to_token[indices]\n",
    "        return [self.idx_to_token[index] for index in indices]\n",
    "\n",
    "    @property\n",
    "    def unk(self):  # 未知词元的索引为0\n",
    "        return 0\n",
    "\n",
    "    @property\n",
    "    def token_freqs(self):\n",
    "        return self._token_freqs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('<unk>', 0), ('the', 1), ('and', 2), ('of', 3), ('i', 4), ('a', 5), ('to', 6), ('in', 7), ('was', 8), ('that', 9)]\n",
      "文本: ['the', 'project', 'gutenberg', 'ebook', 'of', 'the', 'time', 'machine', 'by', 'h', 'g', 'wells']\n",
      "索引: [1, 53, 44, 314, 3, 1, 19, 46, 33, 1163, 1164, 360]\n",
      "文本: ['title', 'the', 'time', 'machine']\n",
      "索引: [2445, 1, 19, 46]\n"
     ]
    }
   ],
   "source": [
    "vocab = Vocab(tokens)\n",
    "print(list(vocab.token_to_idx.items())[:10])\n",
    "for i in [0, 10]:\n",
    "    print('文本:', tokens[i])\n",
    "    print('索引:', vocab[tokens[i]])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. 将文本转换为数字索引序列，方便模型操作"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [],
   "source": [
    "def load_corpus(max_tokens=-1):\n",
    "    \"\"\"返回时光机器数据集的词元索引列表和词表\"\"\"\n",
    "    lines = read_txt(\"../data/时间机器.txt\")\n",
    "    tokens = tokenize(lines)\n",
    "    vocab = Vocab(tokens)\n",
    "    # 因为时光机器数据集中的每个文本行不一定是一个句子或一个段落，所以将所有文本行展平到一个列表中\n",
    "    corpus = [vocab[token] for line in tokens for token in line]\n",
    "    if max_tokens > 0:  # 即最大文本数，避免内存溢出\n",
    "        corpus = corpus[:max_tokens]\n",
    "    return corpus, vocab"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [
    {
     "data": {
      "text/plain": "(36019, 4942)"
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus, vocab = load_corpus()\n",
    "len(corpus), len(vocab)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 简单示例\n",
    "1分词 2创建词表 3词嵌入表示"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "life is not easy for any of us we must work and above all we must believe in ourselves we must believe that each one of us is able to do some thing well and that we must work until we succeed \n",
      "词表  {'able': 0, 'above': 1, 'all': 2, 'and': 3, 'any': 4, 'believe': 5, 'do': 6, 'each': 7, 'easy': 8, 'for': 9, 'in': 10, 'is': 11, 'life': 12, 'must': 13, 'not': 14, 'of': 15, 'one': 16, 'ourselves': 17, 'some': 18, 'succeed': 19, 'that': 20, 'thing': 21, 'to': 22, 'until': 23, 'us': 24, 'we': 25, 'well': 26, 'work': 27}\n",
      "s映射  [12, 11, 14, 8, 9, 4, 15, 24, 25, 13, 27, 3, 1, 2, 25, 13, 5, 10, 17, 25, 13, 5, 20, 7, 16, 15, 24, 11, 0, 22, 6, 18, 21, 26, 3, 20, 25, 13, 27, 23, 25, 19]\n"
     ]
    }
   ],
   "source": [
    "s = 'Life is not easy for any of us.We must work,and above all we must believe in ourselves.We must believe that each one of us is able to do some thing well.And that we must work until we succeed.'\n",
    "for c in string.punctuation: # 去除标点符号，替换为空格，并全部小写\n",
    "    s = s.replace(c, ' ').lower()\n",
    "print(s)\n",
    "vocab = dict((word, index) for index, word in enumerate(np.unique(s.split())))  # 创建词表\n",
    "print(\"词表 \",vocab)\n",
    "s = [vocab.get(w) for w in s.split()]  # 将s映射为词表表示\n",
    "print(\"s映射 \",s)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 独热编码"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 转换为独热编码\n",
    "b = np.zeros((len(s), len(vocab)))\n",
    "for index, i in enumerate(s):\n",
    "    b[index, i] = 1\n",
    "b[:5]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 词嵌入表示"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 1.0550,  0.4054,  0.5093, -0.9537,  0.4279,  0.6771, -0.4031, -1.0563,\n          0.3883, -0.1057],\n        [ 0.8376,  2.4439,  0.7858, -0.2589,  0.6345,  0.0563,  1.0418,  0.0950,\n          1.4032,  0.4671],\n        [ 0.0708,  1.0927,  0.7107, -1.4244,  1.1544,  2.1377, -0.1187,  0.5330,\n          1.4930,  0.4909],\n        [ 0.9150, -2.1348,  0.6800,  0.6468, -2.0456, -1.7328,  0.2601, -2.1585,\n          1.8926,  0.5428],\n        [-0.8145, -0.9149,  0.0263, -2.4155,  0.3824, -0.1913, -0.0900, -0.1396,\n          0.6837,  1.7483]], grad_fn=<SliceBackward0>)"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "em = nn.Embedding(len(vocab), 10)  # 将42个单词映射到长度为10的张量\n",
    "s_em = em(torch.LongTensor(s))\n",
    "s_em[:5]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 电影评论分类IMDB\n",
    "电影评论：一个评论，label为消极、积极、未知 为3分类问题\n",
    "参考 https://suool.net/archives/1d3523b.html\n",
    "以下代码 适合torchtext 0.06 版本 pip install torchtext==0.6.0\n",
    "所有数据集都是的子类torchtext.data.Dataset，它们继承自torch.utils.data.Dataset，并且具有split和iters实现的方法。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 数据预处理"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# 1 创建字段  (train.fields查看)\n",
    "TEXT = torchtext.data.Field(lower=True, fix_length=200, batch_first=True)  # 表示评论，填充为200\n",
    "LABEL = torchtext.data.Field(sequential=False)  # 表示标签\n",
    "# 2 加载torchtext内置的IMDB电影评论数据\n",
    "train, test = torchtext.datasets.IMDB.splits(TEXT, LABEL, root=r\"../data\")\n",
    "# 3 构建词表 vocab\n",
    "TEXT.build_vocab(train, max_size=10000, min_freq=10, vectors=None)  # 关注前10000个单词，次数小于10次就扔掉；\n",
    "LABEL.build_vocab(train)\n",
    "# 4 加载数据集\n",
    "train_iter, test_iter = torchtext.data.BucketIterator.splits((train, test), batch_size=4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "TEXT.vocab.freqs  # 查看每个单词出现的频率"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "TEXT.vocab.stoi  # 词表本身 (长度10002， 包括填充值pad 和 unknown)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 200])\n",
      "tensor([[  10,  205,  707, 2154, 7831,  116,    0, 1833,    6, 1008,  607,  248,\n",
      "          133,   45,  798,   21,   24,   15, 1016,    0,  145, 1511,  777,   38,\n",
      "          538,   17, 3150,  669, 1467,   37,    2,  280,  134,    2,    0,   10,\n",
      "            7,   32,  573,  530,    5,    2,    0,  380,   45,    0,  210,    6,\n",
      "          325,    6,    0,   65,    0,  709,   15, 1107, 5394,   12,  114,    3,\n",
      "           56, 1764, 3703, 1329,  221,   16,    2,   84,   12,   14, 1740,    2,\n",
      "           24, 2590,   51,   19,  997,   68,    0,    4,    0, 1610,   44,    3,\n",
      "            0,    5,   30,    5,    2, 8598, 6629,    4,    2,  894,   27,    0,\n",
      "          204, 1013,    2, 1966,    7,  257,   31, 2154, 7428,   96,   44,  173,\n",
      "          721,    2,  132,    4,  553,   66,    0,   10,   14,    3,   20,   17,\n",
      "            3,  152,  489,    4,    3, 1319,   36,  262,   43,    6,  395,    2,\n",
      "          374,    8,    3,    0, 1893,   38, 1672,   44,  267,  296,   18,   23,\n",
      "          264, 1208,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1]])\n",
      "torch.Size([4])\n",
      "tensor([2, 1, 2, 1])\n"
     ]
    }
   ],
   "source": [
    "# 看看长什么样\n",
    "def show_text(dataloader):\n",
    "    b = next(iter(dataloader))\n",
    "    text, label = b.text, b.label\n",
    "    print(text.shape)\n",
    "    print(text[:1])\n",
    "    print(label.shape)\n",
    "    print(label)\n",
    "\n",
    "show_text(train_iter)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 使用预训练的词向量\n",
    "当在特定领域（例如医学和制造业）工作时，存在大量用于训练词向量的数据，此时预训练的词向量将会非常有用。\n",
    "当几乎没有数据时，甚至不能有意义地训练词向量时，就可以使用这些在不同的数据语料库（如维基百科、谷歌新闻和Twitter推文）上训练好的词向量。\n",
    "正确率可能会下降，因为语料库特点不一样；"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# 1 创建字段  (train.fields查看)\n",
    "TEXT = torchtext.data.Field(lower=True, fix_length=200, batch_first=True)  # 表示评论，填充为200\n",
    "LABEL = torchtext.data.Field(sequential=False)  # 表示标签\n",
    "# 2 加载torchtext内置的IMDB电影评论数据\n",
    "train, test = torchtext.datasets.IMDB.splits(TEXT, LABEL, root=r\"../data\")\n",
    "# 3 构建词表 vocab\n",
    "# vectors使用预训练的词向量，使用6B版本的词向量，映射为100维；\n",
    "TEXT.build_vocab(train, max_size=10000, min_freq=10, vectors=GloVe(name='6B', dim=100, cache=r'../data/.vector_cache'))\n",
    "LABEL.build_vocab(train)\n",
    "# 4 加载数据集\n",
    "train_iter, test_iter = torchtext.data.BucketIterator.splits((train, test), batch_size=4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n        [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],\n        ...,\n        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n        [-0.1915, -0.2686,  0.0245,  ..., -0.4086, -0.5865,  0.0474],\n        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]])"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT.vocab.vectors  # 查看映射后的词向量"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 创建模型"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "TextNet1(\n  (em): Embedding(10002, 100)\n  (fc1): Linear(in_features=20000, out_features=1024, bias=True)\n  (fc2): Linear(in_features=1024, out_features=3, bias=True)\n)"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TextNet1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TextNet1, self).__init__()\n",
    "        self.em = nn.Embedding(10002, 100)   # batch*200*100  词嵌入表示 10002个单词映射到100维空间\n",
    "        self.fc1 = nn.Linear(200*100, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 3)  # 3分类问题\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.em(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model = TextNet1()\n",
    "model.to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 预训练模型\n",
    "model = TextNet1()\n",
    "model.em.weight.data = TEXT.vocab.vectors  # 使用预训练的词向量替换em层\n",
    "model.em.weight.requires_grad = False  # 不再训练\n",
    "model.to(device)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam([ param for param in model.parameters() if param.requires_grad == True],  lr=0.001)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 训练模型"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "def fit(epoch, model, trainloader, testloader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    running_loss = 0\n",
    "\n",
    "    model.train()\n",
    "    for b in trainloader:\n",
    "        x, y = b.text, b.label  # 一个批次的数据\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        y_pred = model(x)\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        with torch.no_grad():\n",
    "            y_pred = torch.argmax(y_pred, dim=1)\n",
    "            correct += (y_pred == y).sum().item()\n",
    "            total += y.size(0)\n",
    "            running_loss += loss.item()\n",
    "#    exp_lr_scheduler.step()\n",
    "    epoch_loss = running_loss / len(trainloader.dataset)\n",
    "    epoch_acc = correct / total\n",
    "\n",
    "\n",
    "    test_correct = 0\n",
    "    test_total = 0\n",
    "    test_running_loss = 0\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for b in testloader:\n",
    "            x, y = b.text, b.label\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            y_pred = model(x)\n",
    "            loss = loss_fn(y_pred, y)\n",
    "            y_pred = torch.argmax(y_pred, dim=1)\n",
    "            test_correct += (y_pred == y).sum().item()\n",
    "            test_total += y.size(0)\n",
    "            test_running_loss += loss.item()\n",
    "\n",
    "    epoch_test_loss = test_running_loss / len(testloader.dataset)\n",
    "    epoch_test_acc = test_correct / test_total\n",
    "\n",
    "    print('epoch: ', epoch,\n",
    "          'loss： ', round(epoch_loss, 3),\n",
    "          'accuracy:', round(epoch_acc, 3),\n",
    "          'test_loss： ', round(epoch_test_loss, 3),\n",
    "          'test_accuracy:', round(epoch_test_acc, 3)\n",
    "             )\n",
    "\n",
    "    return epoch_loss, epoch_acc, epoch_test_loss, epoch_test_acc"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0 loss：  0.184 accuracy: 0.555 test_loss：  0.158 test_accuracy: 0.66\n",
      "epoch:  1 loss：  0.123 accuracy: 0.81 test_loss：  0.182 test_accuracy: 0.722\n",
      "epoch:  2 loss：  0.043 accuracy: 0.938 test_loss：  0.18 test_accuracy: 0.755\n",
      "epoch:  3 loss：  0.018 accuracy: 0.973 test_loss：  0.236 test_accuracy: 0.759\n",
      "epoch:  4 loss：  0.01 accuracy: 0.985 test_loss：  0.408 test_accuracy: 0.748\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "train_loss, train_acc, test_loss, test_acc = [], [], [], []\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss, epoch_acc, epoch_test_loss, epoch_test_acc = fit(epoch, model, train_iter, test_iter)\n",
    "    train_loss.append(epoch_loss)\n",
    "    train_acc.append(epoch_acc)\n",
    "    test_loss.append(epoch_test_loss)\n",
    "    test_acc.append(epoch_test_acc)\n",
    "## 简单模型显然不好用，过拟合了"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# RNN循环网络\n",
    "使用IMDB数据集"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "hidden_size = 300\n",
    "embeding_dim = 100"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## GRUCell"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class RNN_Encoder(nn.Module):\n",
    "    \"\"\"对评论(序列)依次读取，并输出最后状态，正常模式\"\"\"\n",
    "    def __init__(self, input_dim, hidden_size):\n",
    "        super(RNN_Encoder, self).__init__()\n",
    "        # self.rnn = nn.RNNCell(input_dim, hidden_size)  # 内置的简单RNN，效果不好\n",
    "        self.rnn = nn.GRUCell(input_dim, hidden_size)  # GRUCell效果很好\n",
    "    def forward(self, inputs):  # inputs代表输入序列，shape=seq*batch*dim=200*batch*100\n",
    "        bz = inputs.shape[1]  # batch_size\n",
    "        ht = torch.zeros((bz, hidden_size)).cuda()  # 初始化hidden\n",
    "        for word in inputs:  # 沿着单词(序列长度)进行展开\n",
    "            ht = self.rnn(word, ht)\n",
    "        # ht是整个序列的最终输出\n",
    "        return ht\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.em = nn.Embedding(10002, embeding_dim)   # 200*batch*100 每次迭代一个单词\n",
    "        self.rnn = RNN_Encoder(embeding_dim, hidden_size)     # batch*300\n",
    "        self.fc1 = nn.Linear(hidden_size, 256)\n",
    "        self.fc2 = nn.Linear(256, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.em(x)\n",
    "        x = self.rnn(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model = Net()\n",
    "model.to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## LSTMCell"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class RNN_Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_size):\n",
    "        super(RNN_Encoder, self).__init__()\n",
    "        self.rnn = nn.LSTMCell(input_dim, hidden_size)\n",
    "    def forward(self, inputs):\n",
    "        bz = inputs.shape[1]\n",
    "        ht = torch.zeros((bz, hidden_size)).cuda()\n",
    "        ct = torch.zeros((bz, hidden_size)).cuda()  # 这里要多初始化一个\n",
    "        for word in inputs:\n",
    "            ht, ct = self.rnn(word, (ht, ct))\n",
    "        return ht, ct\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.em = nn.Embedding(10002, embeding_dim)\n",
    "        self.rnn = RNN_Encoder(embeding_dim, hidden_size)\n",
    "        self.fc1 = nn.Linear(hidden_size, 256)\n",
    "        self.fc2 = nn.Linear(256, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.em(x)\n",
    "        _, x = self.rnn(x)  # 这里用ct表示整个句子的理解\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model = Net()\n",
    "model.to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 使用内置的LSTM API"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.em = nn.Embedding(10002, embeding_dim)   # 200*batch*100\n",
    "        self.rnn = nn.LSTM(embeding_dim, hidden_size)     # batch*300\n",
    "        self.fc1 = nn.Linear(hidden_size, 256)\n",
    "        self.fc2 = nn.Linear(256, 3)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        bz = inputs.shape[1]\n",
    "        h0 = torch.zeros((1, bz, hidden_size)).cuda()\n",
    "        c0 = torch.zeros((1, bz, hidden_size)).cuda()\n",
    "        x = self.em(inputs)\n",
    "        r_o, _ = self.rnn(x, (h0, c0))  # 输出所有的输出\n",
    "        r_o = r_o[-1]  # 选择最后的输出作为下一层的输入\n",
    "        x = F.relu(self.fc1(r_o))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 注意力机制 Transformer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 创建模型\n",
    "其他使用IMDB代码"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "hidden_size = 300\n",
    "embeding_dim = 100\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=200):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.em = nn.Embedding(10002, embeding_dim)   # 200*batch*100\n",
    "        self.pos = PositionalEncoding(embeding_dim)\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=embeding_dim, nhead=5)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=6)\n",
    "        self.fc1 = nn.Linear(200, 256)\n",
    "        self.fc2 = nn.Linear(256, 3)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.em(inputs)\n",
    "        x = self.pos(x)\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = x.permute(1, 0, 2)\n",
    "        x = torch.sum(x, dim=-1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}