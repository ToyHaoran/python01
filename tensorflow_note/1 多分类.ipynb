{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras import models, layers, regularizers, losses, Model, callbacks\n",
    "from tensorflow.keras import datasets\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "# from sklearn import datasets\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "import os"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 鸢尾花识别 底层\n",
    "(有点底层源码的感觉，重点记忆，背下来)\n",
    "\n",
    "利用鸢尾花数据集，实现前向传播、反向传播，可视化loss曲线\n",
    "\n",
    "数据集介绍：共有数据150组，每组包括花尊长、花尊宽、花瓣长、花瓣宽4个输入特征。以及这组特征对应的鸢尾花类别。\n",
    "类别包括(狗尾草鸢尾)(杂色鸢尾)(弗吉尼亚鸢尾)三类，分别用数字0，1，2表示。\n",
    "\n",
    "1 准备数据\n",
    "    数据集读入\n",
    "    数据集乱序\n",
    "    生成训练集和测试集\n",
    "    配成(输入特征，标签)对，每次读入一小撮(batch)\n",
    "2 搭建网络\n",
    "    定义神经网路中所有可训练参数\n",
    "3 参数优化\n",
    "    嵌套循环迭代，with结构更新参数，显示当前loss\n",
    "4 测试效果\n",
    "    计算当前参数前向传播后的准确率，显示当前acc\n",
    "5 acc / loss可视化"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 数据预处理"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# 导入数据，分别为输入特征和标签\n",
    "data = datasets.load_iris().data  # .data返回iris数据集所有输入特征\n",
    "label = datasets.load_iris().target  # .target返回iris数据集所有标签\n",
    "\n",
    "# 随机打乱数据(因为原始数据是顺序的，顺序不打乱会影响准确率)\n",
    "# seed: 随机数种子，可以为任何整数，当设置之后，每次生成的随机数都一样，都是按116号方式随机的；\n",
    "# 如果不指定，每次训练结果都不一样；如果三个地方不相同，输入特征和标签就对应不上了；\n",
    "np.random.seed(116)\n",
    "np.random.shuffle(data)\n",
    "np.random.seed(116)\n",
    "np.random.shuffle(label)\n",
    "tf.random.set_seed(116)\n",
    "\n",
    "# 将打乱后的数据集分割为训练集和测试集，训练集为前120行，测试集为后30行\n",
    "x_train = data[:-30]\n",
    "y_train = label[:-30]\n",
    "x_test = data[-30:]\n",
    "y_test = label[-30:]\n",
    "\n",
    "# 转换x的数据类型，否则后面矩阵相乘时会因数据类型不一致报错\n",
    "x_train = tf.cast(x_train, tf.float32)\n",
    "x_test = tf.cast(x_test, tf.float32)\n",
    "\n",
    "# 使输入特征和标签值一一对应。（把数据集分批次，每个批次batch组数据）\n",
    "train_db = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32)\n",
    "test_db = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(32)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 搭建网络 定义参数"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# 定义神经网络所有可训练参数(用Variable标记)\n",
    "# 4个输入特征，故输入层为4个输入节点；因为3分类，故输出层为3个神经元\n",
    "w1 = tf.Variable(tf.random.truncated_normal([4, 3], stddev=0.1, seed=1))  # 权重4行3列的张量\n",
    "b1 = tf.Variable(tf.random.truncated_normal([3], stddev=0.1, seed=1))  # 偏置\n",
    "\n",
    "# 超参数：不同于权重w和偏置b，需要人工设定；\n",
    "lr = 0.1  # 学习率为0.1 (可使用指数衰减学习率)\n",
    "train_loss_results = []  # 将每轮的loss记录在此列表中，为后续画loss曲线提供数据\n",
    "test_acc = []  # 将每轮的acc记录在此列表中，为后续画acc曲线提供数据\n",
    "epoch = 500  # 循环多少轮\n",
    "loss_all = 0  # 每轮分4个step，loss_all记录四个step生成的4个loss的和"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 开始训练"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss: 0.2821310982108116\n",
      "Test_acc: 0.16666666666666666\n",
      "--------------------------\n",
      "Epoch 1, loss: 0.25459615513682365\n",
      "Test_acc: 0.16666666666666666\n",
      "--------------------------\n",
      "Epoch 2, loss: 0.22570249810814857\n",
      "Test_acc: 0.16666666666666666\n",
      "--------------------------\n",
      "Epoch 3, loss: 0.21028399839997292\n",
      "Test_acc: 0.16666666666666666\n",
      "--------------------------\n",
      "Epoch 4, loss: 0.19942265003919601\n",
      "Test_acc: 0.16666666666666666\n",
      "--------------------------\n",
      "Epoch 5, loss: 0.18873637914657593\n",
      "Test_acc: 0.5\n",
      "--------------------------\n",
      "Epoch 6, loss: 0.17851299047470093\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 7, loss: 0.16922875493764877\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 8, loss: 0.16107673197984695\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 9, loss: 0.15404684841632843\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 10, loss: 0.14802725985646248\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 11, loss: 0.14287303015589714\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 12, loss: 0.1384414155036211\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 13, loss: 0.13460607640445232\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 14, loss: 0.13126072846353054\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 15, loss: 0.12831822223961353\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 16, loss: 0.12570795230567455\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 17, loss: 0.12337299063801765\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 18, loss: 0.12126746587455273\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 19, loss: 0.11935433372855186\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 20, loss: 0.11760355532169342\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 21, loss: 0.11599067971110344\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 22, loss: 0.11449568346142769\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 23, loss: 0.11310207843780518\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 24, loss: 0.11179621890187263\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 25, loss: 0.11056671477854252\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 26, loss: 0.10940408334136009\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 27, loss: 0.10830028541386127\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 28, loss: 0.10724855214357376\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 29, loss: 0.10624313168227673\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 30, loss: 0.1052791029214859\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 31, loss: 0.10435221716761589\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 32, loss: 0.10345886647701263\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 33, loss: 0.10259587876498699\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 34, loss: 0.10176052898168564\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 35, loss: 0.10095042176544666\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 36, loss: 0.10016347281634808\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 37, loss: 0.09939785301685333\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 38, loss: 0.0986519306898117\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 39, loss: 0.09792429022490978\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 40, loss: 0.09721364825963974\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 41, loss: 0.09651889838278294\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 42, loss: 0.09583901055157185\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 43, loss: 0.09517310559749603\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 44, loss: 0.09452036581933498\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 45, loss: 0.0938800759613514\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 46, loss: 0.09325156174600124\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 47, loss: 0.09263424947857857\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 48, loss: 0.09202760085463524\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 49, loss: 0.09143111668527126\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 50, loss: 0.09084436297416687\n",
      "Test_acc: 0.5666666666666667\n",
      "--------------------------\n",
      "Epoch 51, loss: 0.09026693925261497\n",
      "Test_acc: 0.5666666666666667\n",
      "--------------------------\n",
      "Epoch 52, loss: 0.08969846926629543\n",
      "Test_acc: 0.5666666666666667\n",
      "--------------------------\n",
      "Epoch 53, loss: 0.08913860656321049\n",
      "Test_acc: 0.6\n",
      "--------------------------\n",
      "Epoch 54, loss: 0.08858705498278141\n",
      "Test_acc: 0.6\n",
      "--------------------------\n",
      "Epoch 55, loss: 0.08804351650178432\n",
      "Test_acc: 0.6\n",
      "--------------------------\n",
      "Epoch 56, loss: 0.08750772476196289\n",
      "Test_acc: 0.6\n",
      "--------------------------\n",
      "Epoch 57, loss: 0.0869794450700283\n",
      "Test_acc: 0.6\n",
      "--------------------------\n",
      "Epoch 58, loss: 0.08645843528211117\n",
      "Test_acc: 0.6\n",
      "--------------------------\n",
      "Epoch 59, loss: 0.08594449050724506\n",
      "Test_acc: 0.6\n",
      "--------------------------\n",
      "Epoch 60, loss: 0.08543741703033447\n",
      "Test_acc: 0.6\n",
      "--------------------------\n",
      "Epoch 61, loss: 0.08493701927363873\n",
      "Test_acc: 0.6\n",
      "--------------------------\n",
      "Epoch 62, loss: 0.08444313891232014\n",
      "Test_acc: 0.6333333333333333\n",
      "--------------------------\n",
      "Epoch 63, loss: 0.08395560272037983\n",
      "Test_acc: 0.6333333333333333\n",
      "--------------------------\n",
      "Epoch 64, loss: 0.08347426354885101\n",
      "Test_acc: 0.6333333333333333\n",
      "--------------------------\n",
      "Epoch 65, loss: 0.08299898356199265\n",
      "Test_acc: 0.6333333333333333\n",
      "--------------------------\n",
      "Epoch 66, loss: 0.08252961561083794\n",
      "Test_acc: 0.6333333333333333\n",
      "--------------------------\n",
      "Epoch 67, loss: 0.08206603862345219\n",
      "Test_acc: 0.6333333333333333\n",
      "--------------------------\n",
      "Epoch 68, loss: 0.08160812221467495\n",
      "Test_acc: 0.6333333333333333\n",
      "--------------------------\n",
      "Epoch 69, loss: 0.08115577697753906\n",
      "Test_acc: 0.6333333333333333\n",
      "--------------------------\n",
      "Epoch 70, loss: 0.08070887625217438\n",
      "Test_acc: 0.6333333333333333\n",
      "--------------------------\n",
      "Epoch 71, loss: 0.08026730827987194\n",
      "Test_acc: 0.6333333333333333\n",
      "--------------------------\n",
      "Epoch 72, loss: 0.07983098551630974\n",
      "Test_acc: 0.6666666666666666\n",
      "--------------------------\n",
      "Epoch 73, loss: 0.07939981296658516\n",
      "Test_acc: 0.6666666666666666\n",
      "--------------------------\n",
      "Epoch 74, loss: 0.0789736919105053\n",
      "Test_acc: 0.6666666666666666\n",
      "--------------------------\n",
      "Epoch 75, loss: 0.07855254225432873\n",
      "Test_acc: 0.7\n",
      "--------------------------\n",
      "Epoch 76, loss: 0.07813626900315285\n",
      "Test_acc: 0.7\n",
      "--------------------------\n",
      "Epoch 77, loss: 0.07772481068968773\n",
      "Test_acc: 0.7\n",
      "--------------------------\n",
      "Epoch 78, loss: 0.07731806673109531\n",
      "Test_acc: 0.7\n",
      "--------------------------\n",
      "Epoch 79, loss: 0.07691597566008568\n",
      "Test_acc: 0.7\n",
      "--------------------------\n",
      "Epoch 80, loss: 0.07651844993233681\n",
      "Test_acc: 0.7\n",
      "--------------------------\n",
      "Epoch 81, loss: 0.07612543925642967\n",
      "Test_acc: 0.7333333333333333\n",
      "--------------------------\n",
      "Epoch 82, loss: 0.0757368616759777\n",
      "Test_acc: 0.7333333333333333\n",
      "--------------------------\n",
      "Epoch 83, loss: 0.07535265013575554\n",
      "Test_acc: 0.7333333333333333\n",
      "--------------------------\n",
      "Epoch 84, loss: 0.07497274875640869\n",
      "Test_acc: 0.7333333333333333\n",
      "--------------------------\n",
      "Epoch 85, loss: 0.07459708396345377\n",
      "Test_acc: 0.7666666666666667\n",
      "--------------------------\n",
      "Epoch 86, loss: 0.074225596152246\n",
      "Test_acc: 0.7666666666666667\n",
      "--------------------------\n",
      "Epoch 87, loss: 0.07385822664946318\n",
      "Test_acc: 0.7666666666666667\n",
      "--------------------------\n",
      "Epoch 88, loss: 0.07349492143839598\n",
      "Test_acc: 0.7666666666666667\n",
      "--------------------------\n",
      "Epoch 89, loss: 0.07313562091439962\n",
      "Test_acc: 0.7666666666666667\n",
      "--------------------------\n",
      "Epoch 90, loss: 0.07278026547282934\n",
      "Test_acc: 0.7666666666666667\n",
      "--------------------------\n",
      "Epoch 91, loss: 0.0724287973716855\n",
      "Test_acc: 0.7666666666666667\n",
      "--------------------------\n",
      "Epoch 92, loss: 0.07208118494600058\n",
      "Test_acc: 0.7666666666666667\n",
      "--------------------------\n",
      "Epoch 93, loss: 0.07173734344542027\n",
      "Test_acc: 0.8\n",
      "--------------------------\n",
      "Epoch 94, loss: 0.07139724027365446\n",
      "Test_acc: 0.8\n",
      "--------------------------\n",
      "Epoch 95, loss: 0.07106082234531641\n",
      "Test_acc: 0.8\n",
      "--------------------------\n",
      "Epoch 96, loss: 0.07072803378105164\n",
      "Test_acc: 0.8\n",
      "--------------------------\n",
      "Epoch 97, loss: 0.0703988391906023\n",
      "Test_acc: 0.8\n",
      "--------------------------\n",
      "Epoch 98, loss: 0.07007317896932364\n",
      "Test_acc: 0.8333333333333334\n",
      "--------------------------\n",
      "Epoch 99, loss: 0.0697510140016675\n",
      "Test_acc: 0.8666666666666667\n",
      "--------------------------\n",
      "Epoch 100, loss: 0.06943229213356972\n",
      "Test_acc: 0.8666666666666667\n",
      "--------------------------\n",
      "Epoch 101, loss: 0.06911696959286928\n",
      "Test_acc: 0.8666666666666667\n",
      "--------------------------\n",
      "Epoch 102, loss: 0.06880500447005033\n",
      "Test_acc: 0.8666666666666667\n",
      "--------------------------\n",
      "Epoch 103, loss: 0.068496348336339\n",
      "Test_acc: 0.8666666666666667\n",
      "--------------------------\n",
      "Epoch 104, loss: 0.06819095648825169\n",
      "Test_acc: 0.8666666666666667\n",
      "--------------------------\n",
      "Epoch 105, loss: 0.06788879167288542\n",
      "Test_acc: 0.8666666666666667\n",
      "--------------------------\n",
      "Epoch 106, loss: 0.06758981943130493\n",
      "Test_acc: 0.8666666666666667\n",
      "--------------------------\n",
      "Epoch 107, loss: 0.0672939820215106\n",
      "Test_acc: 0.9\n",
      "--------------------------\n",
      "Epoch 108, loss: 0.06700124125927687\n",
      "Test_acc: 0.9\n",
      "--------------------------\n",
      "Epoch 109, loss: 0.0667115617543459\n",
      "Test_acc: 0.9\n",
      "--------------------------\n",
      "Epoch 110, loss: 0.06642490904778242\n",
      "Test_acc: 0.9\n",
      "--------------------------\n",
      "Epoch 111, loss: 0.06614123564213514\n",
      "Test_acc: 0.9\n",
      "--------------------------\n",
      "Epoch 112, loss: 0.0658605070784688\n",
      "Test_acc: 0.9\n",
      "--------------------------\n",
      "Epoch 113, loss: 0.06558268051594496\n",
      "Test_acc: 0.9\n",
      "--------------------------\n",
      "Epoch 114, loss: 0.06530772615224123\n",
      "Test_acc: 0.9\n",
      "--------------------------\n",
      "Epoch 115, loss: 0.06503560487180948\n",
      "Test_acc: 0.9\n",
      "--------------------------\n",
      "Epoch 116, loss: 0.06476627103984356\n",
      "Test_acc: 0.9\n",
      "--------------------------\n",
      "Epoch 117, loss: 0.06449970323592424\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 118, loss: 0.06423585396260023\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 119, loss: 0.06397469434887171\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 120, loss: 0.06371618993580341\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 121, loss: 0.06346031092107296\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 122, loss: 0.06320700887590647\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 123, loss: 0.06295627448707819\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 124, loss: 0.0627080425620079\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 125, loss: 0.06246231310069561\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 126, loss: 0.06221904046833515\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 127, loss: 0.061978183686733246\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 128, loss: 0.06173973251134157\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 129, loss: 0.06150364316999912\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 130, loss: 0.061269884929060936\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 131, loss: 0.06103843171149492\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 132, loss: 0.06080925837159157\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 133, loss: 0.06058232951909304\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 134, loss: 0.06035762187093496\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 135, loss: 0.060135108418762684\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 136, loss: 0.05991474911570549\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 137, loss: 0.05969652533531189\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 138, loss: 0.05948041286319494\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 139, loss: 0.05926638375967741\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 140, loss: 0.05905440915375948\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 141, loss: 0.058844463899731636\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 142, loss: 0.05863652843981981\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 143, loss: 0.058430567383766174\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 144, loss: 0.05822655465453863\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 145, loss: 0.05802448093891144\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 146, loss: 0.057824309915304184\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 147, loss: 0.057626024819910526\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 148, loss: 0.0574295949190855\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 149, loss: 0.0572349950671196\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 150, loss: 0.05704221222549677\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 151, loss: 0.05685121566057205\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 152, loss: 0.05666199512779713\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 153, loss: 0.05647451803088188\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 154, loss: 0.056288767606019974\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 155, loss: 0.05610471125692129\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 156, loss: 0.055922345258295536\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 157, loss: 0.0557416332885623\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 158, loss: 0.05556256230920553\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 159, loss: 0.05538512021303177\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 160, loss: 0.05520927347242832\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 161, loss: 0.0550350034609437\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 162, loss: 0.05486230552196503\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 163, loss: 0.05469114240258932\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 164, loss: 0.05452151130884886\n",
      "Test_acc: 0.9666666666666667\n",
      "--------------------------\n",
      "Epoch 165, loss: 0.05435337871313095\n",
      "Test_acc: 0.9666666666666667\n",
      "--------------------------\n",
      "Epoch 166, loss: 0.054186731576919556\n",
      "Test_acc: 0.9666666666666667\n",
      "--------------------------\n",
      "Epoch 167, loss: 0.054021554067730904\n",
      "Test_acc: 0.9666666666666667\n",
      "--------------------------\n",
      "Epoch 168, loss: 0.0538578312844038\n",
      "Test_acc: 0.9666666666666667\n",
      "--------------------------\n",
      "Epoch 169, loss: 0.053695546463131905\n",
      "Test_acc: 0.9666666666666667\n",
      "--------------------------\n",
      "Epoch 170, loss: 0.0535346744582057\n",
      "Test_acc: 0.9666666666666667\n",
      "--------------------------\n",
      "Epoch 171, loss: 0.05337520595639944\n",
      "Test_acc: 0.9666666666666667\n",
      "--------------------------\n",
      "Epoch 172, loss: 0.05321711394935846\n",
      "Test_acc: 0.9666666666666667\n",
      "--------------------------\n",
      "Epoch 173, loss: 0.05306038819253445\n",
      "Test_acc: 0.9666666666666667\n",
      "--------------------------\n",
      "Epoch 174, loss: 0.05290501844137907\n",
      "Test_acc: 0.9666666666666667\n",
      "--------------------------\n",
      "Epoch 175, loss: 0.05275098513811827\n",
      "Test_acc: 0.9666666666666667\n",
      "--------------------------\n",
      "Epoch 176, loss: 0.0525982566177845\n",
      "Test_acc: 0.9666666666666667\n",
      "--------------------------\n",
      "Epoch 177, loss: 0.052446840330958366\n",
      "Test_acc: 0.9666666666666667\n",
      "--------------------------\n",
      "Epoch 178, loss: 0.05229670833796263\n",
      "Test_acc: 0.9666666666666667\n",
      "--------------------------\n",
      "Epoch 179, loss: 0.05214785039424896\n",
      "Test_acc: 0.9666666666666667\n",
      "--------------------------\n",
      "Epoch 180, loss: 0.052000245079398155\n",
      "Test_acc: 0.9666666666666667\n",
      "--------------------------\n",
      "Epoch 181, loss: 0.05185388680547476\n",
      "Test_acc: 0.9666666666666667\n",
      "--------------------------\n",
      "Epoch 182, loss: 0.05170875135809183\n",
      "Test_acc: 0.9666666666666667\n",
      "--------------------------\n",
      "Epoch 183, loss: 0.05156483128666878\n",
      "Test_acc: 0.9666666666666667\n",
      "--------------------------\n",
      "Epoch 184, loss: 0.051422109827399254\n",
      "Test_acc: 0.9666666666666667\n",
      "--------------------------\n",
      "Epoch 185, loss: 0.05128058139234781\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 186, loss: 0.05114021245390177\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 187, loss: 0.051001012325286865\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 188, loss: 0.05086293909698725\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 189, loss: 0.05072600580751896\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 190, loss: 0.050590199418365955\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 191, loss: 0.050455489195883274\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 192, loss: 0.05032187420874834\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 193, loss: 0.05018933489918709\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 194, loss: 0.050057861022651196\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 195, loss: 0.04992745537310839\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 196, loss: 0.04979808162897825\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 197, loss: 0.04966974165290594\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 198, loss: 0.04954242613166571\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 199, loss: 0.04941611271351576\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 200, loss: 0.04929080419242382\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 201, loss: 0.049166472628712654\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 202, loss: 0.04904312454164028\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 203, loss: 0.048920731991529465\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 204, loss: 0.04879929404705763\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 205, loss: 0.04867880139499903\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 206, loss: 0.04855923820286989\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 207, loss: 0.048440598882734776\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 208, loss: 0.04832287319004536\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 209, loss: 0.048206047154963017\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 210, loss: 0.04809010960161686\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 211, loss: 0.04797505587339401\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 212, loss: 0.047860877588391304\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 213, loss: 0.04774755984544754\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 214, loss: 0.0476350924000144\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 215, loss: 0.047523465007543564\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 216, loss: 0.0474126823246479\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 217, loss: 0.04730272479355335\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 218, loss: 0.047193579375743866\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 219, loss: 0.04708524886518717\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 220, loss: 0.04697770718485117\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 221, loss: 0.04687096644192934\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 222, loss: 0.04676500428467989\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 223, loss: 0.046659816056489944\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 224, loss: 0.046555389650166035\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 225, loss: 0.04645173158496618\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 226, loss: 0.046348826959729195\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 227, loss: 0.04624664783477783\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 228, loss: 0.04614521563053131\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 229, loss: 0.04604450520128012\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 230, loss: 0.04594451282173395\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 231, loss: 0.04584523383527994\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 232, loss: 0.045746659860014915\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 233, loss: 0.045648783445358276\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 234, loss: 0.045551598072052\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 235, loss: 0.04545509070158005\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 236, loss: 0.045359269715845585\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 237, loss: 0.0452641025185585\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 238, loss: 0.04516960587352514\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 239, loss: 0.04507576394826174\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 240, loss: 0.04498256929218769\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 241, loss: 0.044890016317367554\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 242, loss: 0.044798108749091625\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 243, loss: 0.04470681492239237\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 244, loss: 0.04461614973843098\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 245, loss: 0.04452611040323973\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 246, loss: 0.04443667363375425\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 247, loss: 0.044347841292619705\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 248, loss: 0.04425961058586836\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 249, loss: 0.044171969406306744\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 250, loss: 0.04408490750938654\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 251, loss: 0.04399843979626894\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 252, loss: 0.043912542052567005\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 253, loss: 0.043827205896377563\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 254, loss: 0.043742443434894085\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 255, loss: 0.043658239766955376\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 256, loss: 0.04357459023594856\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 257, loss: 0.043491488322615623\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 258, loss: 0.04340891819447279\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 259, loss: 0.04332689568400383\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 260, loss: 0.043245404958724976\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 261, loss: 0.043164435774087906\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 262, loss: 0.04308399744331837\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 263, loss: 0.043004066683351994\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 264, loss: 0.04292465094476938\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 265, loss: 0.04284574184566736\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 266, loss: 0.04276733845472336\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 267, loss: 0.042689427733421326\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 268, loss: 0.042612007819116116\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 269, loss: 0.042535082437098026\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 270, loss: 0.04245864413678646\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 271, loss: 0.0423826826736331\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 272, loss: 0.04230719245970249\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 273, loss: 0.042232176288962364\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 274, loss: 0.04215762112289667\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 275, loss: 0.042083531618118286\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 276, loss: 0.04200989846140146\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 277, loss: 0.04193671327084303\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 278, loss: 0.04186398349702358\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 279, loss: 0.04179169982671738\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 280, loss: 0.041719856671988964\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 281, loss: 0.04164844285696745\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 282, loss: 0.0415774704888463\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 283, loss: 0.041506923735141754\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 284, loss: 0.041436802595853806\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 285, loss: 0.041367098689079285\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 286, loss: 0.04129782132804394\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 287, loss: 0.04122895281761885\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 288, loss: 0.04116049222648144\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 289, loss: 0.04109244793653488\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 290, loss: 0.04102479945868254\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 291, loss: 0.04095755238085985\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 292, loss: 0.04089069180190563\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 293, loss: 0.04082423448562622\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 294, loss: 0.040758166462183\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 295, loss: 0.040692479349672794\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 296, loss: 0.04062717128545046\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 297, loss: 0.040562248788774014\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 298, loss: 0.040497696958482265\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 299, loss: 0.04043351951986551\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 300, loss: 0.04036971367895603\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 301, loss: 0.04030627105385065\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 302, loss: 0.040243194438517094\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 303, loss: 0.04018046800047159\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 304, loss: 0.04011810338124633\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 305, loss: 0.04005609406158328\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 306, loss: 0.039994440507143736\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 307, loss: 0.03993312222883105\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 308, loss: 0.039872155059129\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 309, loss: 0.03981153108179569\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 310, loss: 0.03975124331191182\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 311, loss: 0.03969129594042897\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 312, loss: 0.039631680119782686\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 313, loss: 0.03957238281145692\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 314, loss: 0.03951342590153217\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 315, loss: 0.03945479029789567\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 316, loss: 0.03939648391678929\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 317, loss: 0.03933848859742284\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 318, loss: 0.03928081085905433\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 319, loss: 0.039223446510732174\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 320, loss: 0.03916640114039183\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 321, loss: 0.03910966124385595\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 322, loss: 0.039053221233189106\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 323, loss: 0.038997095078229904\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 324, loss: 0.03894126741215587\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 325, loss: 0.038885737769305706\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 326, loss: 0.03883049916476011\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 327, loss: 0.038775558583438396\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 328, loss: 0.03872091369703412\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 329, loss: 0.038666554260998964\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 330, loss: 0.0386124849319458\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 331, loss: 0.03855870245024562\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 332, loss: 0.03850520076230168\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 333, loss: 0.03845197660848498\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 334, loss: 0.03839903557673097\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 335, loss: 0.03834636835381389\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 336, loss: 0.03829397959634662\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 337, loss: 0.03824185347184539\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 338, loss: 0.03819000208750367\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 339, loss: 0.038138415198773146\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 340, loss: 0.03808710025623441\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 341, loss: 0.03803604422137141\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 342, loss: 0.03798525780439377\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 343, loss: 0.037934715393930674\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 344, loss: 0.03788443887606263\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 345, loss: 0.03783441847190261\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 346, loss: 0.03778464952483773\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 347, loss: 0.03773513389751315\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 348, loss: 0.03768586507067084\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 349, loss: 0.03763684304431081\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 350, loss: 0.03758806874975562\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 351, loss: 0.03753953706473112\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 352, loss: 0.037491250317543745\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 353, loss: 0.037443204782903194\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 354, loss: 0.03739539487287402\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 355, loss: 0.0373478215187788\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 356, loss: 0.03730047307908535\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 357, loss: 0.03725337516516447\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 358, loss: 0.03720649844035506\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 359, loss: 0.037159846629947424\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 360, loss: 0.03711342951282859\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 361, loss: 0.03706724522635341\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 362, loss: 0.03702127141878009\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 363, loss: 0.036975529976189137\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 364, loss: 0.03693000553175807\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 365, loss: 0.0368847013451159\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 366, loss: 0.036839612759649754\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 367, loss: 0.03679474862292409\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 368, loss: 0.03675009310245514\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 369, loss: 0.036705652717500925\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 370, loss: 0.03666142327710986\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 371, loss: 0.036617396865040064\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 372, loss: 0.03657358651980758\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 373, loss: 0.03652997920289636\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 374, loss: 0.036486583296209574\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 375, loss: 0.03644339041784406\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 376, loss: 0.03640039311721921\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 377, loss: 0.0363576035015285\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 378, loss: 0.036315012723207474\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 379, loss: 0.03627262031659484\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 380, loss: 0.036230423022061586\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 381, loss: 0.0361884250305593\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 382, loss: 0.03614661889150739\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 383, loss: 0.03610500367358327\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 384, loss: 0.03606357052922249\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 385, loss: 0.036022343672811985\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 386, loss: 0.035981299821287394\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 387, loss: 0.03594044363126159\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 388, loss: 0.03589977417141199\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 389, loss: 0.035859286319464445\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 390, loss: 0.03581898985430598\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 391, loss: 0.03577886521816254\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 392, loss: 0.03573893057182431\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 393, loss: 0.03569917054846883\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 394, loss: 0.03565958747640252\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 395, loss: 0.0356201883405447\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 396, loss: 0.03558096243068576\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 397, loss: 0.03554190881550312\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 398, loss: 0.03550303215160966\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 399, loss: 0.03546432638540864\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 400, loss: 0.035425789188593626\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 401, loss: 0.03538742894306779\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 402, loss: 0.03534922935068607\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 403, loss: 0.03531120624393225\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 404, loss: 0.035273338202387094\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 405, loss: 0.03523564711213112\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 406, loss: 0.03519811388105154\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 407, loss: 0.03516074409708381\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 408, loss: 0.03512354660779238\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 409, loss: 0.035086494870483875\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 410, loss: 0.03504961961880326\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 411, loss: 0.03501288779079914\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 412, loss: 0.03497632360085845\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 413, loss: 0.03493990935385227\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 414, loss: 0.03490366041660309\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 415, loss: 0.03486755723133683\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 416, loss: 0.034831615164875984\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 417, loss: 0.03479582304134965\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 418, loss: 0.03476017666980624\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 419, loss: 0.034724689088761806\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 420, loss: 0.03468935191631317\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 421, loss: 0.03465416422113776\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 422, loss: 0.03461912041530013\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 423, loss: 0.0345842270180583\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 424, loss: 0.03454947331920266\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 425, loss: 0.03451487002894282\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 426, loss: 0.034480408765375614\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 427, loss: 0.03444609511643648\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 428, loss: 0.03441191930323839\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 429, loss: 0.034377886448055506\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 430, loss: 0.0343439974822104\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 431, loss: 0.034310245886445045\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 432, loss: 0.034276632592082024\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 433, loss: 0.03424314968287945\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 434, loss: 0.03420981438830495\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 435, loss: 0.03417661041021347\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 436, loss: 0.03414354659616947\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 437, loss: 0.03411061270162463\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 438, loss: 0.03407781617715955\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 439, loss: 0.03404514491558075\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 440, loss: 0.034012613352388144\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 441, loss: 0.03398020705208182\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 442, loss: 0.03394793486222625\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 443, loss: 0.033915786538273096\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 444, loss: 0.033883772790431976\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 445, loss: 0.03385189129039645\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 446, loss: 0.033820131327956915\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 447, loss: 0.03378849755972624\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 448, loss: 0.03375699184834957\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 449, loss: 0.033725605346262455\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 450, loss: 0.03369434783235192\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 451, loss: 0.03366321325302124\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 452, loss: 0.03363220160827041\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 453, loss: 0.0336013101041317\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 454, loss: 0.03357054525986314\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 455, loss: 0.033539887983351946\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 456, loss: 0.033509367145597935\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 457, loss: 0.03347895201295614\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 458, loss: 0.033448657020926476\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 459, loss: 0.033418480306863785\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 460, loss: 0.03338842652738094\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 461, loss: 0.03335848497226834\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 462, loss: 0.03332865657284856\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 463, loss: 0.033298938535153866\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 464, loss: 0.03326933877542615\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 465, loss: 0.033239856362342834\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 466, loss: 0.03321048244833946\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 467, loss: 0.03318121982738376\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 468, loss: 0.033152075950056314\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 469, loss: 0.03312302986159921\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 470, loss: 0.03309411043301225\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 471, loss: 0.033065286464989185\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 472, loss: 0.03303657751530409\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 473, loss: 0.033007977064698935\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 474, loss: 0.03297947999089956\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 475, loss: 0.03295109001919627\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 476, loss: 0.032922806683927774\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 477, loss: 0.03289463324472308\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 478, loss: 0.03286656038835645\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 479, loss: 0.03283859696239233\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 480, loss: 0.032810729928314686\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 481, loss: 0.03278297185897827\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 482, loss: 0.03275531157851219\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 483, loss: 0.03272775514051318\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 484, loss: 0.03270029602572322\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 485, loss: 0.03267294820398092\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 486, loss: 0.03264569165185094\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 487, loss: 0.03261853335425258\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 488, loss: 0.03259148774668574\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 489, loss: 0.03256453201174736\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 490, loss: 0.032537673600018024\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 491, loss: 0.032510916236788034\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 492, loss: 0.032484252005815506\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 493, loss: 0.032457681372761726\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 494, loss: 0.03243121085688472\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 495, loss: 0.032404832541942596\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 496, loss: 0.032378556206822395\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 497, loss: 0.03235236741602421\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 498, loss: 0.03232627036049962\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 499, loss: 0.032300276681780815\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "total_time 3.4300405979156494\n"
     ]
    }
   ],
   "source": [
    "now_time = time.time()  # 记录开始时间\n",
    "for epoch in range(epoch):  # 数据集级别的循环，每个epoch循环一次数据集\n",
    "    for step, (x_train, y_train) in enumerate(train_db):  # batch级别的循环 ，每个step循环一个batch\n",
    "        with tf.GradientTape() as tape:  # with结构记录梯度信息\n",
    "            # 前向传播过程 计算y\n",
    "            y = tf.matmul(x_train, w1) + b1  # y为预测结果\n",
    "            y = tf.nn.softmax(y)  # 使输出y符合概率分布(此操作后与独热码同量级，可相减求loss)\n",
    "            y_ = tf.one_hot(y_train, depth=3)  # 将标签值转换为独热码格式，方便计算loss和accuracy\n",
    "            loss = tf.reduce_mean(tf.square(y_ - y))  # 采用均方误差损失函数mse = mean(sum(y-out)^2)\n",
    "            # loss_regularization = tf.reduce_sum([tf.nn.l2_loss(w1)])  # 添加l2正则化防止过拟合\n",
    "            # loss = loss + 0.03 * loss_regularization  # REGULARIZER = 0.03\n",
    "            loss_all += loss.numpy()  # 将每个step计算出的loss累加，为后续求loss平均值提供数据，这样计算的loss更准确\n",
    "\n",
    "        # 计算loss对各个参数的梯度\n",
    "        grads = tape.gradient(loss, [w1, b1])\n",
    "\n",
    "        # 实现梯度更新 w1 = w1 - lr * w1_grad    b = b - lr * b_grad\n",
    "        # 这里可以修改为其他优化器如SGDM、AdaGrad等(tensorflow已封装)\n",
    "        w1.assign_sub(lr * grads[0])  # 参数w1自更新\n",
    "        b1.assign_sub(lr * grads[1])  # 参数b自更新\n",
    "\n",
    "    # 每个epoch，打印loss信息\n",
    "    print(\"Epoch {}, loss: {}\".format(epoch, loss_all / 4))\n",
    "    train_loss_results.append(loss_all / 4)  # 将4个step的loss求平均记录在此变量中\n",
    "    loss_all = 0  # loss_all归零，为记录下一个epoch的loss做准备\n",
    "\n",
    "    # 测试部分\n",
    "    # total_correct为预测对的样本个数, total_number为测试的总样本数，将这两个变量都初始化为0\n",
    "    total_correct, total_number = 0, 0\n",
    "    for x_test, y_test in test_db:\n",
    "        y = tf.matmul(x_test, w1) + b1  # 使用更新后的参数进行预测\n",
    "        y = tf.nn.softmax(y)\n",
    "        pred = tf.argmax(y, axis=1)  # 返回y中最大值的索引，即预测的分类\n",
    "        pred = tf.cast(pred, dtype=y_test.dtype)  # 将pred转换为test_label的数据类型\n",
    "        # 若分类正确，则correct=1，否则为0，将bool型的结果转换为int型\n",
    "        correct = tf.cast(tf.equal(pred, y_test), dtype=tf.int32)\n",
    "        correct = tf.reduce_sum(correct)  # 将每个batch的correct数加起来\n",
    "        total_correct += int(correct)  # 将所有batch中的correct数加起来\n",
    "        # total_number为测试的总样本数，也就是test_data的行数，shape[0]返回变量的行数\n",
    "        total_number += x_test.shape[0]\n",
    "\n",
    "    # 总的准确率等于total_correct/total_number\n",
    "    acc = total_correct / total_number\n",
    "    test_acc.append(acc)\n",
    "    print(\"Test_acc:\", acc)\n",
    "    print(\"--------------------------\")\n",
    "total_time = time.time() - now_time  # 记录结束时间\n",
    "print(\"total_time\", total_time)  # 打印训练时间"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## acc loss 可视化"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 720x360 with 2 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "size = 5  # 单张图片大小\n",
    "plt.figure(figsize=(size*2, size*1))  # 图像窗口\n",
    "\n",
    "# 绘制 loss 曲线\n",
    "plt.subplot(1,2,1)\n",
    "plt.title('Loss Function Curve')  # 图片标题\n",
    "plt.xlabel('Epoch')  # x轴变量名称\n",
    "plt.ylabel('Loss')  # y轴变量名称\n",
    "plt.plot(train_loss_results, label=\"$Loss$\")  # 逐点画出train_loss_results值并连线，连线图标是Loss\n",
    "plt.legend()  # 画出曲线图标\n",
    "\n",
    "# 绘制 Accuracy 曲线\n",
    "plt.subplot(1,2,2)\n",
    "plt.title('Acc Curve')  # 图片标题\n",
    "plt.xlabel('Epoch')  # x轴变量名称\n",
    "plt.ylabel('Acc')  # y轴变量名称\n",
    "plt.plot(test_acc, label=\"$Accuracy$\")  # 逐点画出test_acc值并连线，连线图标是Accuracy\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 鸢尾花识别 标准模型\n",
    "六步法搭建神经网络\n",
    "第一步：import相关模块，如import tensorflow as tf。\n",
    "第二步：指定输入网络的训练集和测试集，如指定训练集的输入x_train和标签y_train，测试集的输入x_test和标签y_test。\n",
    "第三步：逐层搭建网络结构，model = models.Sequential()。\n",
    "第四步：在model.compile()中配置训练方法，选择训练时使用的优化器、损失函数和最终评价指标。\n",
    "第五步：在model.fit()中执行训练过程，告知训练集和测试集的输入值和标签、每个batch的大小和数据集的迭代次数epoch。\n",
    "第六步：使用model.summary()打印网络结构，统计参数数目。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 数据预处理"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 准备数据\n",
    "x_train = datasets.load_iris().data\n",
    "y_train = datasets.load_iris().target\n",
    "\n",
    "np.random.seed(116)\n",
    "np.random.shuffle(x_train)\n",
    "np.random.seed(116)\n",
    "np.random.shuffle(y_train)\n",
    "tf.random.set_seed(116)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 创建模型\n",
    "用Sequential可以搭建出上层输出就是下层输入的顺序网络结构, 但是无法写出一些带有跳连的非顺序网络结构。这个时候我们可以选择用类class搭建神经网络结构。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class IrisModel(Model):\n",
    "    # 继承Model\n",
    "    def __init__(self):\n",
    "        super(IrisModel, self).__init__()\n",
    "        # 定义网络结构块\n",
    "        self.d1 = Dense(3, activation='softmax', kernel_regularizer=tf.keras.regularizers.l2())\n",
    "\n",
    "    def call(self, x):\n",
    "        y = self.d1(x)  # 调用网络结构块，实现前向传播\n",
    "        return y\n",
    "\n",
    "model = IrisModel()  # 就是把原来的层封装到了类中，与下面等价"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Sequential([网络结构]) 容器，封装了一个神经网络结构。\n",
    "拉直层：layers.Flatten() 这一层不含计算，只是形状转换，把输入特征拉直变成一维数组\n",
    "全连接层：layers.Dense(神经元个数，activation= \"激活函数“，kernel_regularizer=哪种正则化)\n",
    "    activation(字符串)可选: relu、softmax、sigmoid、tanh\n",
    "    kernel_regularizer可选: regularizers.l1()、 regularizers.12()\n",
    "卷积层：layers.Conv2D() 详见卷积神经网络\n",
    "LSTM层；layers.LSTM()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# model = models.Sequential()\n",
    "# model.add(layers.Dense(3, activation='softmax', kernel_regularizer=regularizers.l2()))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 训练模型"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "compile配置神经网络的训练方法，告知训练时选择的优化器、损失函数和评测指标\n",
    "model.compile(optimizer = 优化器, loss = 损失函数, metrics = [\"准确率\"] )\n",
    "Optimizer(优化器)可选:\n",
    "    'sgd' or optimizers.SGD (lr=学习率,momentum=动量参数)\n",
    "    'adagrad' or optimizers.Adagrad (lr=学习率)\n",
    "    'adadelta' or optimizers.Adadelta (lr=学习率)\n",
    "    'adam' or optimizers.Adam (lr=学习率，beta_ 1=0.9, beta_ 2=0.999)\n",
    "loss是(损失函数)可选:\n",
    "    'mse' or losses.MeanSquaredError()\n",
    "    'sparse_categorical_crossentropy' or losses.SparseCategoricalCrossentropy(from_logits=False)\n",
    "    from_logits参数：是否是原始输出，即没有经概率分布的输出。\n",
    "        有些神经网络的输出是经过了softmax等函数的概率分布，有些则不经概率分布直接输出，\n",
    "Metrics(评测指标)可选:\n",
    "    'accuracy' : y_和y都是数值，如y_=[1] y=[1]\n",
    "    'categorical_accuracy' : y_和y都是独热码(概率分布)，如y_ =[0,1,0] y=[0 256.0.695,0.048]\n",
    "    'sparse_categorical_accuracy' : y_是数值，y是独热码(概率分布)，如y_ =[1] y=[0 256,0.695,0.048]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.SGD(lr=0.1),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "              metrics=['sparse_categorical_accuracy'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "model.fit(训练集的输入特征，训练集的标签，\n",
    "        batch_size=每次喂入神经网络的样本数，推荐个数为：2^n\n",
    "        epochs=要迭代多少次数据集\n",
    "        validation_data=(测试集的输入特征，测试集的标签),\n",
    "        validation_split=从训练集划分多少比例给测试集， 和_data二选一\n",
    "        validation_freq =多少次epoch使用测试集验证一次结果)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train, batch_size=32, epochs=500, validation_split=0.2, validation_freq=20)\n",
    "model.summary()  # 打印出网络的结构和参数统计"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 手写数字识别"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 数据预处理"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 加载数据集 train_images.shape=(60000, 28, 28) test_images.shape=(10000, 28, 28)\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "# 正规化：像素在0~255之间，映射到0~1\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 自制数据集\n",
    "即自己写个函数，把mnist.load_data()替换掉，但是同样得到x_train, y_train\n",
    "手写数字图片都是28*28的像素点，每个像素点都是[0, 255]之间的整数；纯黑色=0 纯白色=255\n",
    "label.txt 每行都是：\n",
    "图像名     标签\n",
    "4_8.jpg    8"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_path = './mnist_image_label/mnist_train_jpg_60000/'\n",
    "train_txt = './mnist_image_label/mnist_train_jpg_60000.txt'\n",
    "x_train_savepath = './mnist_image_label/mnist_x_train.npy'\n",
    "y_train_savepath = './mnist_image_label/mnist_y_train.npy'\n",
    "\n",
    "test_path = './mnist_image_label/mnist_test_jpg_10000/'\n",
    "test_txt = './mnist_image_label/mnist_test_jpg_10000.txt'\n",
    "x_test_savepath = './mnist_image_label/mnist_x_test.npy'\n",
    "y_test_savepath = './mnist_image_label/mnist_y_test.npy'\n",
    "\n",
    "def generateds(path, txt):  # (图片路径，标签文件)\n",
    "    f = open(txt, 'r')  # 以只读形式打开txt文件\n",
    "    contents = f.readlines()  # 读取文件中所有行\n",
    "    f.close()  # 关闭txt文件\n",
    "    x, y_ = [], []  # 建立空列表\n",
    "    for content in contents:  # 逐行取出\n",
    "        value = content.split()  # 以空格分开，图片路径为value[0] , 标签为value[1] , 存入列表\n",
    "        img_path = path + value[0]  # 拼出图片路径和文件名\n",
    "        img = Image.open(img_path)  # 读入图片\n",
    "        img = np.array(img.convert('L'))  # 图片变为8位宽灰度值的np.array格式\n",
    "        img = img / 255.  # 数据归一化 （实现预处理）\n",
    "        x.append(img)  # 归一化后的数据，贴到列表x\n",
    "        y_.append(value[1])  # 标签贴到列表y_\n",
    "        print('loading : ' + content)  # 打印状态提示\n",
    "\n",
    "    x = np.array(x)  # 变为np.array格式\n",
    "    y_ = np.array(y_)  # 变为np.array格式\n",
    "    y_ = y_.astype(np.int64)  # 变为64位整型\n",
    "    return x, y_  # 返回输入特征x，返回标签y_\n",
    "\n",
    "# 判断数据集是否存在，存在直接调用，不存在时调用generateds制作数据集\n",
    "if os.path.exists(x_train_savepath) and os.path.exists(y_train_savepath) and os.path.exists(\n",
    "        x_test_savepath) and os.path.exists(y_test_savepath):\n",
    "    print('-------------Load Datasets-----------------')\n",
    "    x_train_save = np.load(x_train_savepath)\n",
    "    y_train = np.load(y_train_savepath)\n",
    "    x_test_save = np.load(x_test_savepath)\n",
    "    y_test = np.load(y_test_savepath)\n",
    "    x_train = np.reshape(x_train_save, (len(x_train_save), 28, 28))\n",
    "    x_test = np.reshape(x_test_save, (len(x_test_save), 28, 28))\n",
    "else:\n",
    "    print('-------------Generate Datasets-----------------')\n",
    "    x_train, y_train = generateds(train_path, train_txt)\n",
    "    x_test, y_test = generateds(test_path, test_txt)\n",
    "\n",
    "    print('-------------Save Datasets-----------------')\n",
    "    x_train_save = np.reshape(x_train, (len(x_train), -1))\n",
    "    x_test_save = np.reshape(x_test, (len(x_test), -1))\n",
    "    np.save(x_train_savepath, x_train_save)\n",
    "    np.save(y_train_savepath, y_train)\n",
    "    np.save(x_test_savepath, x_test_save)\n",
    "    np.save(y_test_savepath, y_test)\n",
    "\n",
    "# 其他同baseline"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 数据增强"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 给数据增加一个维度,从(60000, 28, 28)reshape为(60000, 28, 28, 1)\n",
    "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
    "\n",
    "\"\"\"\n",
    "数据增强：对图像的增强就是对图像进行简单形变，解决因为拍照角度不同等因素造成的影响。\n",
    "image_gen._train = lmageDataGenerator(\n",
    "\trescale =所有数据将乘以该数值\n",
    "\trotation_ range =随机旋转角度数范围\n",
    "\twidth_ shift range =随机宽度偏移量\n",
    "\theight shift range =随机高度偏移量\n",
    "\t水平翻转: horizontal_flip =是否随机水平翻转\n",
    "\t随机缩放: zoom_range =随机缩放的范围[1-n, 1+n] )\n",
    "\"\"\"\n",
    "image_gen_train = ImageDataGenerator(\n",
    "    rescale=1. / 1.,  # 如为图像，分母为255时，可归至0～1\n",
    "    rotation_range=45,  # 随机45度旋转\n",
    "    width_shift_range=.15,  # 宽度偏移\n",
    "    height_shift_range=.15,  # 高度偏移\n",
    "    horizontal_flip=False,  # 水平翻转\n",
    "    zoom_range=0.5  # 将图像随机缩放阈量50％\n",
    ")\n",
    "image_gen_train.fit(x_train)\n",
    "\n",
    "# model.fit(image_gen_train.flow(x_train, y_train, batch_size=32),...  # 数据增强"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 创建模型"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class MnistModel(Model):\n",
    "    def __init__(self):\n",
    "        super(MnistModel, self).__init__()\n",
    "        self.flatten = layers.Flatten()  # 第二维展平 784个元素组成的一维数组\n",
    "        self.d1 = layers.Dense(128, activation='relu')\n",
    "        self.d2 = layers.Dense(10, activation='softmax')  # 多分类问题\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.d1(x)\n",
    "        y = self.d2(x)\n",
    "        return y\n",
    "\n",
    "model = MnistModel()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 断点续训，存取模型\n",
    "load_weights(路径文件名)\n",
    "1 定义存放模型的路径和文件名，命名为ckpt文件\n",
    "2 生成ckpt文件时会同步生成index索引表，所以判断索引表是否存在，来判断是否存在模型参数\n",
    "3 如有索引表，则直接读取ckpt文件中的模型参数"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "checkpoint_save_path = \"./checkpoint/mnist.ckpt\"\n",
    "if os.path.exists(checkpoint_save_path + '.index'):\n",
    "    print('-------------load the model-----------------')\n",
    "    model.load_weights(checkpoint_save_path)\n",
    "cp_callback = callbacks.ModelCheckpoint(filepath=checkpoint_save_path,\n",
    "                                        save_weights_only=True,\n",
    "                                        save_best_only=True)\n",
    "model.fit(x_train, y_train,  epochs=5,\n",
    "          validation_data=(x_test, y_test), validation_freq=1,\n",
    "          callbacks=[cp_callback])  # 保存模型"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 设置损失函数和优化器"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "              metrics=['sparse_categorical_accuracy'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 训练模型"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "history = model.fit(x_train, y_train, batch_size=32, epochs=5,\n",
    "                    validation_data=(x_test, y_test), validation_freq=1)\n",
    "\n",
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 参数提取：把参数存入文本\n",
    "model.trainable_variables：返回模型中可训练的参数"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "np.set_printoptions(threshold=np.inf)  # (threshold=超过多少省略显示)  inf表示无限大\n",
    "print(model.trainable_variables)\n",
    "file = open('./weights.txt', 'w')\n",
    "for v in model.trainable_variables:\n",
    "    file.write(str(v.name) + '\\n')\n",
    "    file.write(str(v.shape) + '\\n')\n",
    "    file.write(str(v.numpy()) + '\\n')\n",
    "file.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## acc loss 可视化\n",
    "history=model.fit(训练集数据，训练集标签，batch_size=, epochs=,\n",
    "    validation_split=用作测试数据的比例,validation_ data=测试集，\n",
    "    validation_freq=测试频率\n",
    "    训练集loss: loss\n",
    "    测试集loss: val_loss\n",
    "    训练集准确率: sparse_categorical_accuracy\n",
    "    测试集准确率: val_sparse_categorical_accuracy)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "acc = history.history['sparse_categorical_accuracy']\n",
    "val_acc = history.history['val_sparse_categorical_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "plt.subplots_adjust(hspace=0.5)  # 调整子图间距\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(acc, label='Training Accuracy')\n",
    "plt.plot(val_acc, label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}