{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.layers import Conv2D, BatchNormalization, Activation, MaxPool2D, AvgPool2D, Dropout, Flatten, Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# cifar10数据集\n",
    "cifar10数据集一共有6万张彩色图片，每张图片有32行32列像素点的红绿蓝三通道数据。\n",
    "提供5万张32*32像素点的十分类彩色图片和标签，用于训练。\n",
    "提供1万张32*32像素点的十分类彩色图片和标签，用于测试。\n",
    "十个分类分别是：飞机、汽车、鸟、猫、鹿、狗、青蛙、马、船和卡车，分别对应标签0、1、2、3一直到9"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "np.set_printoptions(threshold=np.inf)\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "class Baseline(Model):\n",
    "    def __init__(self):\n",
    "        super(Baseline, self).__init__()\n",
    "        # 一层卷积层\n",
    "        \"\"\"\n",
    "        tf描述卷积层 layers.Conv2D (\n",
    "            filters =卷积核个数，\n",
    "            kernel_size =卷积核尺寸(核高h,核宽w)， 或正方形写核长整数\n",
    "            strides =滑动步长(纵向步长h，横向步长w)，或横纵向相同写步长整数，默认1\n",
    "            padding = \"same\" or \"valid\", # 使用全零填充是\"same\",不使用是\"valid\" (默认)\n",
    "            activation =\"relu\" or \"sigmoid\" or \"tanh\" or \"softmax\"等，# 如有BN此处不写\n",
    "            input_shape = (高，宽，通道数)  # 输入特征图维度，可省略\n",
    "            )\n",
    "        \"\"\"\n",
    "        self.c1 = Conv2D(filters=6, kernel_size=(5, 5), padding='same')  # 卷积层\n",
    "        self.b1 = BatchNormalization()  # BN层\n",
    "        self.a1 = Activation('relu')  # 激活层\n",
    "        \"\"\" 池化层\n",
    "        tf.keras.layers.MaxPool2D, AveragePooling2D(\n",
    "            pool_size=池化核尺寸, # 正方形写核长整数，或(核高h，核宽w)\n",
    "            strides=池化步长, #步长整数，或(纵向步长h,横向步长w)，默认为pool_size\n",
    "            padding='valid'or'same' #使用全零填充是\"same\"，不使用是\"valid\" (默认)\n",
    "        )\n",
    "        \"\"\"\n",
    "        self.p1 = MaxPool2D(pool_size=(2, 2), strides=2, padding='same')  # 池化层\n",
    "        self.d1 = Dropout(0.2)  # dropout层\n",
    "        # 两层 全连接层\n",
    "        self.flatten = Flatten()\n",
    "        self.f1 = Dense(128, activation='relu')\n",
    "        self.d2 = Dropout(0.2)\n",
    "        self.f2 = Dense(10, activation='softmax')\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.c1(x)\n",
    "        x = self.b1(x)\n",
    "        x = self.a1(x)\n",
    "        x = self.p1(x)\n",
    "        x = self.d1(x)\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        x = self.f1(x)\n",
    "        x = self.d2(x)\n",
    "        y = self.f2(x)\n",
    "        return y\n",
    "\n",
    "model = Baseline()  # 基础CNN模型\n",
    "# model = LeNet5()\n",
    "# model = Inception10(num_blocks=2, num_classes=10)  # Block数是2，block_0和block_1; 网络10分类\n",
    "# model = ResNet18([2, 2, 2, 2])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "              metrics=['sparse_categorical_accuracy'])\n",
    "\n",
    "checkpoint_save_path = \"Baseline.ckpt\"\n",
    "# checkpoint_save_path = \"./checkpoint/LeNet5.ckpt\"  # 这里不同模型需要修改一下\n",
    "if os.path.exists(checkpoint_save_path + '.index'):\n",
    "    print('-------------load the model-----------------')\n",
    "    model.load_weights(checkpoint_save_path)\n",
    "\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_save_path,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 save_best_only=True)\n",
    "\n",
    "history = model.fit(x_train, y_train, batch_size=32, epochs=5,\n",
    "                    validation_data=(x_test, y_test), validation_freq=1,\n",
    "                    callbacks=[cp_callback])\n",
    "model.summary()\n",
    "\n",
    "# print(model.trainable_variables)\n",
    "\"\"\"\n",
    "baseline/conv2d/kernel:0 (5, 5, 3, 6)记录了第层网络用的5*5*3的卷积核，一共6个，下边给出了这6个卷积核中的所有参数W；\n",
    "baseline/conv2d/bias:0 (6,)这里记录了6个卷积核各自的偏置项b，每个卷积核一个 b，6个卷积核共有6个偏置6 ；\n",
    "baseline/batch_normalization/gamma:0 (6,)，这里记录了BN操作中的缩放因子γ，每个卷积核一个γ，一个6个γ；\n",
    "baseline/batch_normalization/beta:0 (6,)，里记录了BN操作中的偏移因子β，每个卷积核一个β，一个6个β；\n",
    "baseline/dense/kernel:0 (1536, 128)，这里记录了第一层全链接网络，1536 行、128列的线上的权量w；\n",
    "baseline/dense/bias:0 (128,)，这里记录了第一层全连接网络128个偏置b；\n",
    "baseline/dense_1/kernel:0 (128, 10)，这里记录了第二层全链接网络，128行、10列的线上的权量w；\n",
    "baseline/dense_1/bias:0 (10,)，这里记录了第二层全连接网络10个偏置b。\n",
    "\"\"\"\n",
    "file = open('weights.txt', 'w')\n",
    "for v in model.trainable_variables:\n",
    "    file.write(str(v.name) + '\\n')\n",
    "    file.write(str(v.shape) + '\\n')\n",
    "    file.write(str(v.numpy()) + '\\n')\n",
    "file.close()\n",
    "\n",
    "###############################################    show   ###############################################\n",
    "\n",
    "# 显示训练集和验证集的acc和loss曲线\n",
    "acc = history.history['sparse_categorical_accuracy']\n",
    "val_acc = history.history['val_sparse_categorical_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(acc, label='Training Accuracy')\n",
    "plt.plot(val_acc, label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 常见卷积模型"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## LeNet5"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class LeNet5(Model):\n",
    "    def __init__(self):\n",
    "        super(LeNet5, self).__init__()\n",
    "        self.c1 = Conv2D(filters=6, kernel_size=(5, 5), activation='sigmoid')\n",
    "        self.p1 = MaxPool2D(pool_size=(2, 2), strides=2)\n",
    "        self.c2 = Conv2D(filters=16, kernel_size=(5, 5), activation='sigmoid')\n",
    "        self.p2 = MaxPool2D(pool_size=(2, 2), strides=2)\n",
    "\n",
    "        self.flatten = Flatten()\n",
    "        self.f1 = Dense(120, activation='sigmoid')\n",
    "        self.f2 = Dense(84, activation='sigmoid')\n",
    "        self.f3 = Dense(10, activation='softmax')\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.c1(x)\n",
    "        x = self.p1(x)\n",
    "\n",
    "        x = self.c2(x)\n",
    "        x = self.p2(x)\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        x = self.f1(x)\n",
    "        x = self.f2(x)\n",
    "        y = self.f3(x)\n",
    "        return y\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## AlexNet8"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class AlexNet8(Model):\n",
    "    def __init__(self):\n",
    "        super(AlexNet8, self).__init__()\n",
    "        self.c1 = Conv2D(filters=96, kernel_size=(3, 3))\n",
    "        self.b1 = BatchNormalization()\n",
    "        self.a1 = Activation('relu')\n",
    "        self.p1 = MaxPool2D(pool_size=(3, 3), strides=2)\n",
    "\n",
    "        self.c2 = Conv2D(filters=256, kernel_size=(3, 3))\n",
    "        self.b2 = BatchNormalization()\n",
    "        self.a2 = Activation('relu')\n",
    "        self.p2 = MaxPool2D(pool_size=(3, 3), strides=2)\n",
    "\n",
    "        self.c3 = Conv2D(filters=384, kernel_size=(3, 3), padding='same',\n",
    "                         activation='relu')\n",
    "\n",
    "        self.c4 = Conv2D(filters=384, kernel_size=(3, 3), padding='same',\n",
    "                         activation='relu')\n",
    "\n",
    "        self.c5 = Conv2D(filters=256, kernel_size=(3, 3), padding='same',\n",
    "                         activation='relu')\n",
    "        self.p3 = MaxPool2D(pool_size=(3, 3), strides=2)\n",
    "\n",
    "        self.flatten = Flatten()\n",
    "        self.f1 = Dense(2048, activation='relu')\n",
    "        self.d1 = Dropout(0.5)\n",
    "        self.f2 = Dense(2048, activation='relu')\n",
    "        self.d2 = Dropout(0.5)\n",
    "        self.f3 = Dense(10, activation='softmax')\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.c1(x)\n",
    "        x = self.b1(x)\n",
    "        x = self.a1(x)\n",
    "        x = self.p1(x)\n",
    "\n",
    "        x = self.c2(x)\n",
    "        x = self.b2(x)\n",
    "        x = self.a2(x)\n",
    "        x = self.p2(x)\n",
    "\n",
    "        x = self.c3(x)\n",
    "\n",
    "        x = self.c4(x)\n",
    "\n",
    "        x = self.c5(x)\n",
    "        x = self.p3(x)\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        x = self.f1(x)\n",
    "        x = self.d1(x)\n",
    "        x = self.f2(x)\n",
    "        x = self.d2(x)\n",
    "        y = self.f3(x)\n",
    "        return y"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## VGG16"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class VGG16(Model):\n",
    "    def __init__(self):\n",
    "        super(VGG16, self).__init__()\n",
    "        self.c1 = Conv2D(filters=64, kernel_size=(3, 3), padding='same')  # 卷积层1\n",
    "        self.b1 = BatchNormalization()  # BN层1\n",
    "        self.a1 = Activation('relu')  # 激活层1\n",
    "        self.c2 = Conv2D(filters=64, kernel_size=(3, 3), padding='same', )\n",
    "        self.b2 = BatchNormalization()  # BN层1\n",
    "        self.a2 = Activation('relu')  # 激活层1\n",
    "        self.p1 = MaxPool2D(pool_size=(2, 2), strides=2, padding='same')\n",
    "        self.d1 = Dropout(0.2)  # dropout层\n",
    "\n",
    "        self.c3 = Conv2D(filters=128, kernel_size=(3, 3), padding='same')\n",
    "        self.b3 = BatchNormalization()  # BN层1\n",
    "        self.a3 = Activation('relu')  # 激活层1\n",
    "        self.c4 = Conv2D(filters=128, kernel_size=(3, 3), padding='same')\n",
    "        self.b4 = BatchNormalization()  # BN层1\n",
    "        self.a4 = Activation('relu')  # 激活层1\n",
    "        self.p2 = MaxPool2D(pool_size=(2, 2), strides=2, padding='same')\n",
    "        self.d2 = Dropout(0.2)  # dropout层\n",
    "\n",
    "        self.c5 = Conv2D(filters=256, kernel_size=(3, 3), padding='same')\n",
    "        self.b5 = BatchNormalization()  # BN层1\n",
    "        self.a5 = Activation('relu')  # 激活层1\n",
    "        self.c6 = Conv2D(filters=256, kernel_size=(3, 3), padding='same')\n",
    "        self.b6 = BatchNormalization()  # BN层1\n",
    "        self.a6 = Activation('relu')  # 激活层1\n",
    "        self.c7 = Conv2D(filters=256, kernel_size=(3, 3), padding='same')\n",
    "        self.b7 = BatchNormalization()\n",
    "        self.a7 = Activation('relu')\n",
    "        self.p3 = MaxPool2D(pool_size=(2, 2), strides=2, padding='same')\n",
    "        self.d3 = Dropout(0.2)\n",
    "\n",
    "        self.c8 = Conv2D(filters=512, kernel_size=(3, 3), padding='same')\n",
    "        self.b8 = BatchNormalization()  # BN层1\n",
    "        self.a8 = Activation('relu')  # 激活层1\n",
    "        self.c9 = Conv2D(filters=512, kernel_size=(3, 3), padding='same')\n",
    "        self.b9 = BatchNormalization()  # BN层1\n",
    "        self.a9 = Activation('relu')  # 激活层1\n",
    "        self.c10 = Conv2D(filters=512, kernel_size=(3, 3), padding='same')\n",
    "        self.b10 = BatchNormalization()\n",
    "        self.a10 = Activation('relu')\n",
    "        self.p4 = MaxPool2D(pool_size=(2, 2), strides=2, padding='same')\n",
    "        self.d4 = Dropout(0.2)\n",
    "\n",
    "        self.c11 = Conv2D(filters=512, kernel_size=(3, 3), padding='same')\n",
    "        self.b11 = BatchNormalization()  # BN层1\n",
    "        self.a11 = Activation('relu')  # 激活层1\n",
    "        self.c12 = Conv2D(filters=512, kernel_size=(3, 3), padding='same')\n",
    "        self.b12 = BatchNormalization()  # BN层1\n",
    "        self.a12 = Activation('relu')  # 激活层1\n",
    "        self.c13 = Conv2D(filters=512, kernel_size=(3, 3), padding='same')\n",
    "        self.b13 = BatchNormalization()\n",
    "        self.a13 = Activation('relu')\n",
    "        self.p5 = MaxPool2D(pool_size=(2, 2), strides=2, padding='same')\n",
    "        self.d5 = Dropout(0.2)\n",
    "\n",
    "        self.flatten = Flatten()\n",
    "        self.f1 = Dense(512, activation='relu')\n",
    "        self.d6 = Dropout(0.2)\n",
    "        self.f2 = Dense(512, activation='relu')\n",
    "        self.d7 = Dropout(0.2)\n",
    "        self.f3 = Dense(10, activation='softmax')\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.c1(x)\n",
    "        x = self.b1(x)\n",
    "        x = self.a1(x)\n",
    "        x = self.c2(x)\n",
    "        x = self.b2(x)\n",
    "        x = self.a2(x)\n",
    "        x = self.p1(x)\n",
    "        x = self.d1(x)\n",
    "\n",
    "        x = self.c3(x)\n",
    "        x = self.b3(x)\n",
    "        x = self.a3(x)\n",
    "        x = self.c4(x)\n",
    "        x = self.b4(x)\n",
    "        x = self.a4(x)\n",
    "        x = self.p2(x)\n",
    "        x = self.d2(x)\n",
    "\n",
    "        x = self.c5(x)\n",
    "        x = self.b5(x)\n",
    "        x = self.a5(x)\n",
    "        x = self.c6(x)\n",
    "        x = self.b6(x)\n",
    "        x = self.a6(x)\n",
    "        x = self.c7(x)\n",
    "        x = self.b7(x)\n",
    "        x = self.a7(x)\n",
    "        x = self.p3(x)\n",
    "        x = self.d3(x)\n",
    "\n",
    "        x = self.c8(x)\n",
    "        x = self.b8(x)\n",
    "        x = self.a8(x)\n",
    "        x = self.c9(x)\n",
    "        x = self.b9(x)\n",
    "        x = self.a9(x)\n",
    "        x = self.c10(x)\n",
    "        x = self.b10(x)\n",
    "        x = self.a10(x)\n",
    "        x = self.p4(x)\n",
    "        x = self.d4(x)\n",
    "\n",
    "        x = self.c11(x)\n",
    "        x = self.b11(x)\n",
    "        x = self.a11(x)\n",
    "        x = self.c12(x)\n",
    "        x = self.b12(x)\n",
    "        x = self.a12(x)\n",
    "        x = self.c13(x)\n",
    "        x = self.b13(x)\n",
    "        x = self.a13(x)\n",
    "        x = self.p5(x)\n",
    "        x = self.d5(x)\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        x = self.f1(x)\n",
    "        x = self.d6(x)\n",
    "        x = self.f2(x)\n",
    "        x = self.d7(x)\n",
    "        y = self.f3(x)\n",
    "        return y"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Inception10"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class ConvBNRelu(Model):\n",
    "    \"\"\"\n",
    "     卷积连接器把四个分支按照深度方向堆叠在一起，构成Inception结构块的输出，\n",
    "     由于Inception结构块中的卷积操作均采用了CBA结构，即先卷积再BN再采用relu激活函数，\n",
    "     所以将其定义成一个新的类ConvBNRelu，减少代码长度，增加可读性。\n",
    "    \"\"\"\n",
    "    def __init__(self, ch, kernelsz=3, strides=1, padding='same'):\n",
    "        \"\"\"\n",
    "        :param ch: 代表特征图的通道数，也即卷积核个数;\n",
    "        :param kernelsz: 代表卷积核尺寸;\n",
    "        :param strides: 代表卷积步长;\n",
    "        :param padding: 代表是否进行全零填充。\n",
    "        \"\"\"\n",
    "        super(ConvBNRelu, self).__init__()\n",
    "        self.model = tf.keras.models.Sequential([\n",
    "            Conv2D(ch, kernelsz, strides=strides, padding=padding),\n",
    "            BatchNormalization(),\n",
    "            Activation('relu')\n",
    "        ])\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.model(x, training=False) #在training=False时，BN通过整个训练集计算均值、方差去做批归一化，training=True时，通过当前batch的均值、方差去做批归一化。推理时 training=False效果好\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class InceptionBlk(Model):\n",
    "    \"\"\"\n",
    "    完成ConvBNRelu之后，就可以开始构建 InceptionNet 的基本单元了，\n",
    "    同样利用 class 定义的方式，定义一个新的 InceptionBlk类。\n",
    "     InceptionNet 网络的主体就是由其基本单元构成的，有了Inception结构块后，\n",
    "     就可以搭建出一个精简版本的InceptionNet，网络共有10层，其模型结构如图\n",
    "    \"\"\"\n",
    "    def __init__(self, ch, strides=1):\n",
    "        \"\"\" 与 ConvBNRelu 类中一致\n",
    "        :param ch:  仍代表通道数\n",
    "        :param strides: 代表卷积步长\n",
    "        \"\"\"\n",
    "        super(InceptionBlk, self).__init__()\n",
    "        self.ch = ch\n",
    "        self.strides = strides\n",
    "        self.c1 = ConvBNRelu(ch, kernelsz=1, strides=strides)\n",
    "        self.c2_1 = ConvBNRelu(ch, kernelsz=1, strides=strides)\n",
    "        self.c2_2 = ConvBNRelu(ch, kernelsz=3, strides=1)\n",
    "        self.c3_1 = ConvBNRelu(ch, kernelsz=1, strides=strides)\n",
    "        self.c3_2 = ConvBNRelu(ch, kernelsz=5, strides=1)\n",
    "        self.p4_1 = MaxPool2D(3, strides=1, padding='same')\n",
    "        self.c4_2 = ConvBNRelu(ch, kernelsz=1, strides=strides)\n",
    "\n",
    "    def call(self, x):\n",
    "        x1 = self.c1(x)\n",
    "        x2_1 = self.c2_1(x)\n",
    "        x2_2 = self.c2_2(x2_1)\n",
    "        x3_1 = self.c3_1(x)\n",
    "        x3_2 = self.c3_2(x3_1)\n",
    "        x4_1 = self.p4_1(x)\n",
    "        x4_2 = self.c4_2(x4_1)\n",
    "        \"\"\"\n",
    "        concat along axis=channel\n",
    "        函数将四个输出按照深度方向连接在一起，x1、x2_2、x3_2、x4_2 分别代表四列输出，\n",
    "        结合结构图和代码很容易看出二者的对应关系。\n",
    "        \"\"\"\n",
    "        x = tf.concat([x1, x2_2, x3_2, x4_2], axis=3)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Inception10(Model):\n",
    "    \"\"\"\n",
    "    InceptionNet 网络的主体就是由其基本单元构成的，有了Inception结构块后，就可以搭建出一个精简版本的InceptionNet，\n",
    "    网络共有10层，其模型结构如图\n",
    "    第一层采用16个3*3卷积核，步长为1，全零填充，BN操作，rule激活。\n",
    "    随后是4个Inception结构块顺序相连，每两个Inception结构块组成block，\n",
    "        每个block中的第一个Inception结构块，卷积步长是2，第二个Inception结构块，卷积步长是1，\n",
    "        这使得第一个Inception结构块输出特征图尺寸减半，因此把输出特征图深度加深，尽可能保证特征抽取中信息的承载量一致。\n",
    "    block_0设置的通道数是16，经过了四个分支，输出的深度为4*16=64；\n",
    "    在self.out_channels *= 2给通道数加倍了，所以block_1通道数是block_0通道数的两倍是32，\n",
    "    经过了四个分支，输出的深度为4*32=128，这128个通道的数据会被送入平均池化，送入10个分类的全连接。\n",
    "    \"\"\"\n",
    "    def __init__(self, num_blocks, num_classes, init_ch=16, **kwargs):\n",
    "        \"\"\"\n",
    "        InceptionNet 网络不再像 VGGNet 一样有三层全连接层(全连接层的参数量占 VGGNet 总参数量的 90 %)，\n",
    "        而是采用“全局平均池化+全连接层”的方式，这减少了大量的参数。\n",
    "        :param num_blocks: 代表 InceptionNet 的 Block 数，每个 Block 由两个基本单元构成；\n",
    "        :param num_classes: 代表分类数，对于 cifar10 数据集来说即为 10;\n",
    "        :param init_ch: 代表初始通道数，也即 InceptionNet 基本单元的初始卷积核个数。\n",
    "        :param kwargs:\n",
    "        \"\"\"\n",
    "        super(Inception10, self).__init__(**kwargs)\n",
    "        self.in_channels = init_ch\n",
    "        self.out_channels = init_ch\n",
    "        self.num_blocks = num_blocks\n",
    "        self.init_ch = init_ch\n",
    "\n",
    "        # 第一层采用16个3*3卷积核，步长为1，全零填充，BN操作，rule激活。\n",
    "        # 设定了默认init_ch=16,默认输出深度是16，\n",
    "        # 定义ConvBNRe lu类的时候，默认卷积核边长是3步长为1全零填充，所以直接调用\n",
    "        self.c1 = ConvBNRelu(init_ch)\n",
    "\n",
    "        # 每个bIock中的第一个Inception结构块，卷积步长是2，\n",
    "        # 第二个Inception结构块，卷积步长是1，\n",
    "        self.blocks = tf.keras.models.Sequential()\n",
    "        for block_id in range(num_blocks):\n",
    "            for layer_id in range(2):\n",
    "                if layer_id == 0:\n",
    "                    block = InceptionBlk(self.out_channels, strides=2)\n",
    "                else:\n",
    "                    block = InceptionBlk(self.out_channels, strides=1)\n",
    "                self.blocks.add(block)\n",
    "            # enlarger out_channels per block\n",
    "\n",
    "            # 给通道数加倍了，所以block_1通道数是block_0通道数的两倍是32\n",
    "            self.out_channels *= 2\n",
    "\n",
    "        self.p1 = GlobalAveragePooling2D()  # 128个通道的数据会被送入平均池化\n",
    "        self.f1 = Dense(num_classes, activation='softmax')  # 送入10个分类的全连接。\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.c1(x)\n",
    "        x = self.blocks(x)\n",
    "        x = self.p1(x)\n",
    "        y = self.f1(x)\n",
    "        return y"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ResNet18"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class ResnetBlock(Model):\n",
    "    def __init__(self, filters, strides=1, residual_path=False):\n",
    "        super(ResnetBlock, self).__init__()\n",
    "        self.filters = filters\n",
    "        self.strides = strides\n",
    "        self.residual_path = residual_path\n",
    "\n",
    "        self.c1 = Conv2D(filters, (3, 3), strides=strides, padding='same', use_bias=False)\n",
    "        self.b1 = BatchNormalization()\n",
    "        self.a1 = Activation('relu')\n",
    "\n",
    "        self.c2 = Conv2D(filters, (3, 3), strides=1, padding='same', use_bias=False)\n",
    "        self.b2 = BatchNormalization()\n",
    "\n",
    "        # residual_path为True时，对输入进行下采样，即用1x1的卷积核做卷积操作，保证x能和F(x)维度相同，顺利相加\n",
    "        if residual_path:\n",
    "            self.down_c1 = Conv2D(filters, (1, 1), strides=strides, padding='same', use_bias=False)\n",
    "            self.down_b1 = BatchNormalization()\n",
    "\n",
    "        self.a2 = Activation('relu')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        residual = inputs  # residual等于输入值本身，即residual=x\n",
    "        # 将输入通过卷积、BN层、激活层，计算F(x)\n",
    "        x = self.c1(inputs)\n",
    "        x = self.b1(x)\n",
    "        x = self.a1(x)\n",
    "\n",
    "        x = self.c2(x)\n",
    "        y = self.b2(x)\n",
    "\n",
    "        if self.residual_path:\n",
    "            residual = self.down_c1(inputs)\n",
    "            residual = self.down_b1(residual)\n",
    "\n",
    "        out = self.a2(y + residual)  # 最后输出的是两部分的和，即F(x)+x或F(x)+Wx,再过激活函数\n",
    "        return out"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class ResNet18(Model):\n",
    "    def __init__(self, block_list, initial_filters=64):  # block_list表示每个block有几个卷积层\n",
    "        super(ResNet18, self).__init__()\n",
    "        self.num_blocks = len(block_list)  # 共有几个block\n",
    "        self.block_list = block_list\n",
    "        self.out_filters = initial_filters\n",
    "        self.c1 = Conv2D(self.out_filters, (3, 3), strides=1, padding='same', use_bias=False)\n",
    "        self.b1 = BatchNormalization()\n",
    "        self.a1 = Activation('relu')\n",
    "        self.blocks = tf.keras.models.Sequential()\n",
    "        # 构建ResNet网络结构\n",
    "        for block_id in range(len(block_list)):  # 第几个resnet block\n",
    "            for layer_id in range(block_list[block_id]):  # 第几个卷积层\n",
    "\n",
    "                if block_id != 0 and layer_id == 0:  # 对除第一个block以外的每个block的输入进行下采样\n",
    "                    block = ResnetBlock(self.out_filters, strides=2, residual_path=True)\n",
    "                else:\n",
    "                    block = ResnetBlock(self.out_filters, residual_path=False)\n",
    "                self.blocks.add(block)  # 将构建好的block加入resnet\n",
    "            self.out_filters *= 2  # 下一个block的卷积核数是上一个block的2倍\n",
    "        self.p1 = tf.keras.layers.GlobalAveragePooling2D()\n",
    "        self.f1 = tf.keras.layers.Dense(10, activation='softmax', kernel_regularizer=tf.keras.regularizers.l2())\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.c1(inputs)\n",
    "        x = self.b1(x)\n",
    "        x = self.a1(x)\n",
    "        x = self.blocks(x)\n",
    "        x = self.p1(x)\n",
    "        y = self.f1(x)\n",
    "        return y"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 结尾"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}